---
title: 'Mirror, mirror on the wall, does my test data reflect my training data at all?'
author: Amanda Peterson
date: '2020-11-08'
description: A closer look at the train-test split 
tags: ["machine learning", "rstats"]
draft: false
code-copy: true
image: mirror_cartoon.jpg
---

```{r setup}
#| include: false

knitr::opts_chunk$set(echo = TRUE)
```

In the paper [Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches](https://arxiv.org/pdf/1907.06902.pdf){target="_blank"}, authors Dacrema, Cremonesi, and Jannach evaluate various neural networks which were designed for recommendation systems and proposed in prominent machine learning journals. A recommendation system is an information filtering system which uses user history (e.g. purchases, page views, clicks, thumbs up/down, etc) to provide personalized item recommendations to users.

The authors point out a few mistakes or "questionable techniques" used in the research they were attempting to reproduce and evaluate. The figure in the article snippet below points out that, for a particular model they were evaluating, the popularity distribution of items assigned to the test and train data splits did not seem to be the same. *Interestingly, this affected the reported results and potentially the claim that the proposed model was a state-of-the-art!* See the article for more details.

------------------------------------------------------------------------


![](images/para1.png){width="48%" fig-align='center'}
![](images/para2.png){width="48%" fig-align='center'}

Snippet of the 2020 paper, "Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches", by Dacrema, Cremonesi, and Jannach


------------------------------------------------------------------------

I thought this was a nice example comparing the distribution of two samples (i.e. comparing the item popularity of the test set versus the training set). Why is this important? Well, because we would like to train our model using data similar to data that the model will score in the future. The test set is used to tell us how good (or bad) our model performs on data that it has not seen before.

The authors mentioned that they used the Gini index to evaluate the distribution of the test set. One might also wonder if a hypothesis test could be used to consider the evidence for or against the training and test sets having the same underlying distribution. Let's look at both approaches.

```{r}
#| include: false

#install.packages(c("devtools", "pins"))
library(pins)
library(tidyverse)
library(gt)
library(cowplot)
library(magrittr)
```

```{r}
#| eval: false
#| include: false

# Read train and test data from github.com/lzheng21/SpectralCF 
# Save in data directory

path <- "https://raw.githubusercontent.com/lzheng21/SpectralCF/master/data/ml-1m/"
url <- str_c(path, "test_user.dat")
con <- pins::pin(url)

test <- read_lines(con) %>%
  as_tibble() %>%
  separate(value,  into = c("user", "item"), extra = "merge") %>%
  separate_rows(item, sep = " ") %>%
  mutate_at(c("user","item"), as.integer)

url <- str_c(path, "train_users.dat")
con <- pins::pin(url)

train <- read_lines(con) %>%
  as_tibble() %>%
  separate(value, into = c("user", "item"), extra = "merge") %>%
  separate_rows(item, sep = " ")  %>%
  mutate_at(c("user","item"), as.integer)

save(test, train, file = "data/train_test.RData")
```

## The data

The train and test data mentioned in the article above are available at [github.com/lzheng21/SpectralCF](https://github.com/lzheng21/SpectralCF/tree/master/data/ml-1m){target="_blank"}. For convenience I also saved them as R dataframes in the [data directory](https://github.com/AmandaRP/AmandaRP.github.io/tree/master/content/post/2020-11-08-mirror-mirror-on-the-wall-does-my-test-data-reflect-my-training-data-at-all/data){target="_blank"} of the GitHub repo associated with this analysis. Using R, the data can be loaded as follows.

```{r class.source = "fold-show"}
#| code-copy: true 
load("data/train_test.RData")
```

Let's take a peek at the training set. The data is composed of two columns containing user ids and item ids. It comes from [MovieLens](https://grouplens.org/datasets/movielens/){target="_blank"}, which provides data about movie watching history. The existence of a user-item pair in the data means that the user interacted with the item (in this case the user watched the movie). A data scientist can use this information as "implicit feedback", inferring that the user "liked" the item.

```{r}
#| echo: false
slice_sample(train, n = 5) %>%
  gt() %>%
  tab_header(title = "Sample of Training Data")
```



```{r}
#| echo: false

train %>% 
  summarize(min_user = min(user), max_user = max(user), num_users = n_distinct(user), min_item = min(item),  max_item = max(item), num_items = n_distinct(item)) %>% 
  gt() %>% 
  tab_header(title = "Summary of Training Data") %>%
  cols_label(min_user = "Min User ID", 
             max_user = "Max User ID",
             num_users = "User Count",
             min_item = "Min Item ID",
             max_item = "Max Item ID",
             num_items = "Item Count",)
```

Next, count the number of user interactions per item in each of the train and test datasets. Only the first 6 rows of the result are shown in the table below.

```{r class.source = 'fold-show'}
#| code-copy: true

item_frequency <- 
  full_join(
    train %>% group_by(item) %>% count() %>% ungroup(),
    test  %>% group_by(item) %>% count() %>% ungroup(),
    by = "item"
  ) %>%
  rename(count_train = n.x, count_test = n.y) %>%
  replace_na(list(count_train = 0, count_test = 0))
```

```{r}
#| echo: false

item_frequency %>% 
  head() %>% 
  gt() %>%
  cols_label(count_train = "Train Count",
             count_test = "Test Count")
```

We can use this information to re-create the plot from the paper:

```{r}
#| echo: true
#| eval: true
#| fig.align: 'center'
#| fig.cap: 'Figure 1: Comparison of the provided test and train data'
#| code-fold: true
#| code-copy: true

item_frequency %>%
  mutate(train = count_train/max(count_train), test = count_test/max(count_test)) %>%
  arrange(desc(count_train)) %>%
  mutate(train_rank = row_number()) %>%
  select(-count_train, -count_test) %>%
  pivot_longer(-c(item, train_rank), names_to = "data", values_to = "num_interactions") %>%
  ggplot(aes(train_rank, num_interactions, group = data, color = data)) +
  geom_line() +
  geom_point() +
  xlab("Item Index") +
  ylab("Normalized number of interactions")
```

I think that looks pretty close to the image in the article! As the authors point out, the items are sorted by their popularity in the training set (in descending order along the horizontal axis). The value of the (normalized) test popularity is also plotted for each item. We can see that the two distributions do not look similar. To be a bit more rigorous we can consider a few statistical procedures.

## Gini Index

The article authors used the Gini index to evaluate the distribution of the provided test set. Let's try it to see how it works. We'll use the [`ineq`](https://CRAN.R-project.org/package=ineq){target="_blank"} R package.

```{r class.source = "fold-show"}
#| code-copy: true

#install.packages("ineq")
library(ineq)
```

The Gini index is a measure of inequality ranging from 0 (equality) to 1 (no equality). For example, if there were no popularity bias in the dataset (i.e. all items had the same number of interactions), then the Gini index would be 0. That's not the case since some movies were watched more than others. Let's look at the Gini index for the provided train and test sets.

```{r class.source = "fold-show"}
#| code-copy: true
# Gini index for provided test set
gini_provided_test <- 
  item_frequency %>% 
  select(-item, -count_train) %>%
  as.matrix() %>%
  ineq(type="Gini")

# Gini index for provided train set
gini_provided_train <- 
  item_frequency %>% 
  select(-item, -count_test) %>%
  as.matrix() %>%
  ineq(type="Gini")

gini_provided_train
gini_provided_test
```

The provided test set has a much higher Gini index than the train dataset (`r round(gini_provided_test, digits = 2)` compared to `r round(gini_provided_train, digits = 2)`). Is such a difference expected for a random train/test split? Keep reading.

## Resampling the test data

Let's try sampling our own test set to see how that affects the Gini index. We'll start with the full dataset, sampling user-item interactions at the same rate as the provided train/test split and stratifying by user.

```{r class.source = "fold-show"}
#| code-copy: true
# Determine percentage of data that should be sampled for test
(pct_test <- nrow(test)/(nrow(test) + nrow(train)))
```

```{r message = FALSE, class.source = "fold-show"}
#| code-copy: true
# Sample test set (stratify by user)
data_full <- bind_rows(train,test)
test_new <- 
  data_full %>% 
  group_by(user) %>%
  slice_sample(prop = pct_test) %>%
  ungroup()

# Put remaining data in train:
train_new <- anti_join(data_full, test_new)

# Recreate our interaction counts for each item:
item_frequency_new <- 
  full_join(
    train_new %>% group_by(item) %>% count() %>% ungroup(),
    test_new  %>% group_by(item) %>% count() %>% ungroup(),
    by = "item"
  ) %>%
  rename(count_train = n.x, count_test = n.y) %>%
  replace_na(list(count_train = 0, count_test = 0))

```

Let's visualize the newly sampled train and test data, which we can compare to Figure 1.


```{r }
#| echo: true
#| fig.align: 'center'
#| fig.cap: 'Figure 2: Comparison of our sampled test and train data'
#| code-fold: true
#| code-copy: true

item_frequency_new %>%
  mutate(train = count_train/max(count_train), test = count_test/max(count_test)) %>%
  arrange(desc(count_train)) %>%
  mutate(train_rank = row_number()) %>%
  select(-count_train, -count_test) %>%
  pivot_longer(-c(item, train_rank), names_to = "data", values_to = "num_interactions") %>%
  ggplot(aes(train_rank, num_interactions, group = data, color = data)) +
  geom_line() +
  geom_point() +
  xlab("Items") +
  ylab("Normalized number of interactions")
```


The test and train popularity distributions look much closer!

## Gini Index Revisited

Now we can revisit the Gini index for the re-sampled train/test split.

```{r class.source = "fold-show"}
#| code-copy: true
# Gini index for our sampled test set:
gini_ours_test <- 
  item_frequency_new %>% 
  select(-item, -count_train) %>%
  as.matrix() %>%
  ineq(type="Gini")
gini_ours_test

# Gini index for our sampled train set:
gini_ours_train <- 
  item_frequency_new %>% 
  select(-item, -count_test) %>%
  as.matrix() %>%
  ineq(type="Gini")

gini_ours_train
```

We see that our sampled test set has a Gini index (`r round(gini_ours_test, digits = 2)`) that is much closer to that of our sampled training dataset (`r round(gini_ours_train, digits = 2)`).

But, how much difference can we expect between the two sampled datasets that are representative of the same distribution? Is the observed difference in the provided datasets likely due to random sampling or some other reason? Let's take a look at a couple of hypothesis tests.

## Hypothesis Testing

### $\chi^2$ Test of Homogeneity

<!-- ### $\chi^2$ Goodness of Fit Test -->

Let's suppose that our null hypothesis is that the train and test sets have the *same* underlying distribution (vs the alternative hypothesis that they do not). We can consider the data as a 2 by `r nrow(item_frequency)` contingency table (since there are `r nrow(item_frequency)` items). One might naively choose to run a $\chi^2$ test to test this hypothesis.

<!-- Let's suppose that our null hypothesis is that the test set is representative of the full dataset (vs the alternative hypothesis that it is not). We can consider the data as a 2 by `r nrow(item_frequency)` contingency table (since there are `r nrow(item_frequency)` items). -->

<!-- One might naively choose to run a $\chi^2$ test to test this hypothesis.  -->

The $\chi^2$ test is most appropriate when the data is not too sparse. Recall that one of the rule-of-thumb assumptions for the $\chi^2$ test is that no more than 20% of the expected cells counts are less than 5. <!-- (Sometimes a slightly different rule of thumb is used: that *all* cell counts have an expected value of *at least* 5.)  -->\
Let's check to see if this assumption is satisfied.

```{r class.source = "fold-show"}
#| code-copy: true
# Check to see percentage of expected cell values are < 5

# Compute the expected values
n_train <- sum(item_frequency$count_train)
n_test <- sum(item_frequency$count_test) 
n <- n_train + n_test
item_frequency %<>% 
  mutate(p = (count_train + count_test)/n,
         e_test = p * n_test,
         e_train = p * n_train)

# Check what percentage of the expected values are less than 5
(pct_expected_small <- sum((item_frequency$e_test < 5) + (item_frequency$e_test < 5))/(2*nrow(item_frequency)))
```

We see here that `r round(100*pct_expected_small, digits = 1)`% of the expected values are very small (less than 5), meaning that the assumption in question is **not** satisfied.

<!-- In addition to this issue, the test and train vectors are not independent. (The test/train split is a random partition of the full dataset. The data assigned to one of the sets is dependent on the data that is sampled for the other.) Independence is another assumption for the $\chi^2$ test. -->

Let's look at an alternative method.

### Fisher's Exact Test

Again consider a 2 by `r nrow(item_frequency)` contingency table composed of the test and train vectors. We can utilize a test that is acceptable for small sample sizes: Fisher's exact test can be used to test the null hypothesis that the two factors of a 2-dimensional contingency table are independent (no relationship) vs the alternative that they are not.

Consider first Fisher's exact test for the original train/test data:

```{r}
#| code-copy: true
item_frequency %>% 
  select(count_train, count_test) %>% 
  fisher.test(simulate.p.value = TRUE)
```

The p-value is very small indicating that there is sufficient evidence that movie popularity is dependent on the dataset (test or train).

Now consider Fisher's exact test for our re-sampled train/test data:

```{r}
#| code-copy: true
item_frequency_new %>% 
  select(count_train, count_test) %>% 
  fisher.test(simulate.p.value = TRUE)
```

In this case the p-value is large meaning that we fail to reject the null hypothesis that movie popularity is independent of the dataset. In other words, we see a similar popularity pattern in both the test and train data sets that we sampled. These results align with our expectation.

<!-- ### Hypothesis testing of high dimensional (and sparse) discrete data -->

<!-- In a [previous blog post](https://amanda.rbind.io/2019/05/24/new-r-package-hddtest/){target="_blank"} I introduced a new method for testing whether two high dimensional mutlinomial vectors have the same underlying probability vector. Compared to the $\chi^2$ test, this method is more robust in the presence of small values. The R method is available via the `hddtest` package available in the [AmandaRP/hddtest](https://github.com/AmandaRP/hddtest){target="_blank"} GitHub repository. -->

<!-- ```{r class.source = "fold-show"} -->

<!-- #devtools::install_github("AmandaRP/hddtest", build = TRUE, build_opts = c("--no-resave-data", "--no-manual")) -->

<!-- library("hddtest") -->

<!-- ``` -->

<!-- We'll assume that the train and test vectors have an underlying multinomial distribution with parameters ($p_{train}$, $n_{train}$) and ($p_{test}$, $n_{test}$) respectively. (See the [Closing Thoughts](#closing) section at the end for a discussion about the multinomial assumption.) We'd like to test the null hypothesis that $p_{train} = p_{test}$ vs the althernative that they are not equal. To do this we might consider using the `multinom.test` function from the `hddtest` package. However, a simulation experiment shows that the [size](https://en.wikipedia.org/wiki/Size_(statistics)) of the test is not well controlled when the data is stratified by user (see the Appendix at the end of the article). In other words, the test is too sensitive.  -->

<!-- Alternatively, we can consider the null hypothesis that $p_{train}$ and $p_{test}$ are within some neighborhood, $\delta$ (vs the alternative that they are not). What value of $\delta$ should be used? Figure 3 below shows the p-value curves for various values of $\delta$, comparing multiple random test/train splits (teal) and also the provided test & train data (red). We won't actually perform the hypothesis test because we've now peaked at our data. However, you can see that the p-value curve for the provided data looks much different than those for the random test/train splits. -->

<!-- <details> -->

<!--   <summary>View code</summary> -->

<!-- </br>  -->

<!-- ```{r, fig.width=8, fig.height=3.5, message = FALSE, warning=FALSE, echo = TRUE, eval = FALSE, fig.cap='Delta in (0, 2800]. Inset image: Zoomed view of delta in (0, 5].'} -->

<!-- num_reps <- 50 -->

<!-- stratified <- TRUE -->

<!-- vecs2Test <- list(matrix(NA, num_reps, nrow(item_frequency)), matrix(NA, num_reps, nrow(item_frequency))) -->

<!-- for(i in 1:num_reps){ -->

<!--   if(stratified){ -->

<!--     test_sim <-  -->

<!--       data_full %>%  -->

<!--       group_by(user) %>% -->

<!--       slice_sample(prop = pct_test) %>% -->

<!--       ungroup() -->

<!--   }else{ -->

<!--     test_sim <-  -->

<!--       data_full %>%  -->

<!--       slice_sample(prop = pct_test)  -->

<!--   } -->

<!--   # Put remaining data in train: -->

<!--   train_sim <- anti_join(data_full, test_sim) -->

<!--   # Recreate our interaction counts for each item: -->

<!--   item_frequency_sim <-  -->

<!--     full_join( -->

<!--       train_sim %>% group_by(item) %>% count() %>% ungroup(), -->

<!--       test_sim  %>% group_by(item) %>% count() %>% ungroup(), -->

<!--       by = "item" -->

<!--     ) %>% -->

<!--     rename(count_train = n.x, count_test = n.y) %>% -->

<!--     replace_na(list(count_train = 0, count_test = 0)) -->

<!--   # Build a matrix of vectors to test -->

<!--   vecs2Test[[1]][i, ] <- item_frequency_sim$count_train -->

<!--   vecs2Test[[2]][i, ] <- item_frequency_sim$count_test -->

<!-- } -->

<!-- #Perform the test: -->

<!-- delta = c(.00000001, .00001, .0001, .001, .01, .1, .5, .75, seq(from=1,to=2800, by = 100) ) -->

<!-- result_sampled <- vecs2Test %>% multinom.neighborhood.test(delta = delta) -->

<!-- p_sampled <- result_sampled$pvalue_delta -->

<!-- result_original <- multinom.neighborhood.test(x = item_frequency$count_train, y = item_frequency$count_test, delta = delta) -->

<!-- p_original <- result_original$pvalue_delta -->

<!-- dat1 <- data.frame(delta=delta, sampled = t(p_sampled), original = t(p_original)) %>% -->

<!--   gather(-delta, key = "replication", value = pvalue) %>%  -->

<!--   mutate("hypothesis" = str_extract(replication,"\\w{1,}")) -->

<!-- p1 <- ggplot(dat1, aes(x = delta, y = pvalue, group = replication, colour = hypothesis)) +  -->

<!--   geom_line() + -->

<!--   geom_hline(yintercept = 0.05, linetype = "dotted") + -->

<!--   annotate("text", x = 300, y = .09, label = "alpha = 0.05") + -->

<!--   labs(y = "p-value", title = "P-Value Curves for Original and Sampled Train/Test Splits", color = "Dataset") +  -->

<!--   #geom_curve(aes(x = 75, y = .71, xend = 61, yend = .6), curvature = -0.3, arrow = arrow(length=unit(2,"mm")), color = "darkgrey") + -->

<!--   theme_classic() + -->

<!--   theme(title = element_text(face = "bold", size = 14), -->

<!--         panel.grid = element_blank()) -->

<!-- delta = c(.00000001, .00001, .0001, .001, .01, .1, .5, .75, 1:5) -->

<!-- result_sampled <- vecs2Test %>% multinom.neighborhood.test(delta = delta) -->

<!-- p_sampled <- result_sampled$pvalue_delta -->

<!-- result_original <- multinom.neighborhood.test(x = item_frequency$count_train, y = item_frequency$count_test, delta = delta) -->

<!-- p_original <- result_original$pvalue_delta -->

<!-- dat2 <- data.frame(delta=delta, sampled = t(p_sampled), original = t(p_original)) %>% -->

<!--   gather(-delta, key = "replication", value = pvalue) %>%  -->

<!--   mutate("hypothesis" = str_extract(replication,"\\w{1,}")) -->

<!-- p2 <- ggplot(dat2, aes(x = delta, y = pvalue, group = replication, colour = hypothesis)) +  -->

<!--   geom_line() + -->

<!--   labs(y = element_blank()) +  -->

<!--   #geom_curve(aes(x = 75, y = .71, xend = 61, yend = .6), curvature = -0.3, arrow = arrow(length=unit(2,"mm")), color = "darkgrey") + -->

<!--   theme_classic() + -->

<!--   theme(title = element_text(face = "bold", size = 14), -->

<!--         panel.grid = element_blank(), -->

<!--         legend.title = element_blank(), -->

<!--         legend.position = "none" -->

<!--         ) -->

<!-- ggdraw(p1) + -->

<!--   draw_plot(p2, .25, .25, .4, .5)  -->

<!-- ``` -->

<!-- </details> -->

<!-- ```{r, fig.width=8, fig.height=3.5, message = FALSE, warning=FALSE, echo = FALSE, fig.cap='Delta in (0, 2800]. Inset image: Zoomed view of delta in (0, 5].'} -->

<!-- num_reps <- 50 -->

<!-- stratified <- TRUE -->

<!-- vecs2Test <- list(matrix(NA, num_reps, nrow(item_frequency)), matrix(NA, num_reps, nrow(item_frequency))) -->

<!-- for(i in 1:num_reps){ -->

<!--   if(stratified){ -->

<!--     test_sim <-  -->

<!--       data_full %>%  -->

<!--       group_by(user) %>% -->

<!--       slice_sample(prop = pct_test) %>% -->

<!--       ungroup() -->

<!--   }else{ -->

<!--     test_sim <-  -->

<!--       data_full %>%  -->

<!--       slice_sample(prop = pct_test)  -->

<!--   } -->

<!--   # Put remaining data in train: -->

<!--   train_sim <- anti_join(data_full, test_sim) -->

<!--   # Recreate our interaction counts for each item: -->

<!--   item_frequency_sim <-  -->

<!--     full_join( -->

<!--       train_sim %>% group_by(item) %>% count() %>% ungroup(), -->

<!--       test_sim  %>% group_by(item) %>% count() %>% ungroup(), -->

<!--       by = "item" -->

<!--     ) %>% -->

<!--     rename(count_train = n.x, count_test = n.y) %>% -->

<!--     replace_na(list(count_train = 0, count_test = 0)) -->

<!--   # Build a matrix of vectors to test -->

<!--   vecs2Test[[1]][i, ] <- item_frequency_sim$count_train -->

<!--   vecs2Test[[2]][i, ] <- item_frequency_sim$count_test -->

<!-- } -->

<!-- #Perform the test: -->

<!-- delta = c(.00000001, .00001, .0001, .001, .01, .1, .5, .75, seq(from=1,to=2800, by = 100) ) -->

<!-- result_sampled <- vecs2Test %>% multinom.neighborhood.test(delta = delta) -->

<!-- p_sampled <- result_sampled$pvalue_delta -->

<!-- result_original <- multinom.neighborhood.test(x = item_frequency$count_train, y = item_frequency$count_test, delta = delta) -->

<!-- p_original <- result_original$pvalue_delta -->

<!-- dat1 <- data.frame(delta=delta, sampled = t(p_sampled), original = t(p_original)) %>% -->

<!--   gather(-delta, key = "replication", value = pvalue) %>%  -->

<!--   mutate("hypothesis" = str_extract(replication,"\\w{1,}")) -->

<!-- p1 <- ggplot(dat1, aes(x = delta, y = pvalue, group = replication, colour = hypothesis)) +  -->

<!--   geom_line() + -->

<!--   geom_hline(yintercept = 0.05, linetype = "dotted") + -->

<!--   annotate("text", x = 300, y = .09, label = "alpha = 0.05") + -->

<!--   labs(y = "p-value", title = "P-Value Curves for Original and Sampled Train/Test Splits", color = "Dataset") +  -->

<!--   #geom_curve(aes(x = 75, y = .71, xend = 61, yend = .6), curvature = -0.3, arrow = arrow(length=unit(2,"mm")), color = "darkgrey") + -->

<!--   theme_classic() + -->

<!--   theme(title = element_text(face = "bold", size = 14), -->

<!--         panel.grid = element_blank()) -->

<!-- delta = c(.00000001, .00001, .0001, .001, .01, .1, .5, .75, 1:5) -->

<!-- result_sampled <- vecs2Test %>% multinom.neighborhood.test(delta = delta) -->

<!-- p_sampled <- result_sampled$pvalue_delta -->

<!-- result_original <- multinom.neighborhood.test(x = item_frequency$count_train, y = item_frequency$count_test, delta = delta) -->

<!-- p_original <- result_original$pvalue_delta -->

<!-- dat2 <- data.frame(delta=delta, sampled = t(p_sampled), original = t(p_original)) %>% -->

<!--   gather(-delta, key = "replication", value = pvalue) %>%  -->

<!--   mutate("hypothesis" = str_extract(replication,"\\w{1,}")) -->

<!-- p2 <- ggplot(dat2, aes(x = delta, y = pvalue, group = replication, colour = hypothesis)) +  -->

<!--   geom_line() + -->

<!--   labs(y = element_blank()) +  -->

<!--   #geom_curve(aes(x = 75, y = .71, xend = 61, yend = .6), curvature = -0.3, arrow = arrow(length=unit(2,"mm")), color = "darkgrey") + -->

<!--   theme_classic() + -->

<!--   theme(title = element_text(face = "bold", size = 14), -->

<!--         panel.grid = element_blank(), -->

<!--         legend.title = element_blank(), -->

<!--         legend.position = "none" -->

<!--         ) -->

<!-- ggdraw(p1) + -->

<!--   draw_plot(p2, .25, .25, .4, .5)  -->

<!-- ``` -->

## Closing Thoughts {#closing}

Using a few methods (vizualization, Gini index, and hypothesis testing) we observed that the provided test data set did not follow the same popularity distribution as the training set.

Another scenario to consider: Should we always expect our train and test data to have the same distribution? What if the test/train split were based on time (i.e. we use earlier data to train the model and use the most recent data to test). How much difference is acceptable? After all, movie popularity changes over time (thinking about our particular application). This might be an interesting topic for a future blog post.

<!-- One might also argue that we should not assume a multinomial distribution for the item counts as we did for the hypothesis testing. If there is correlation between the items (e.g. if the first movie in a series was popular, we might expect the sequel to also do well), then the Dirichlet-multinomial distribution may be a better assumption. In this case, a different hypothesis tests is required. -->

If you would like to provide feedback on this blog post, you can contact me via Twitter (@DrAmandaRP). Thanks for reading!

<!-- <details> -->

<!--   <summary>**Appendix**: Size Study</summary> -->

<!-- To understand the size of the hypothesis test (i.e. the `multinom.test` from the `hddtest` package) for the application of comparing the test and training datasets, let's run a simulation sampling mulitple test/train splits.  -->

<!-- ```{r eval = FALSE, class.source = 'fold-show'} -->

<!-- data_full_sim <- data_full -->

<!-- alpha <- 0.05 -->

<!-- num_reps <- 10000 -->

<!-- stratified <- FALSE -->

<!-- result <- array(NA, dim = num_reps) -->

<!-- for(i in 1:num_reps){ -->

<!--   if(stratified){ -->

<!--     test_sim <-  -->

<!--       data_full_sim %>%  -->

<!--       group_by(user) %>% -->

<!--       slice_sample(prop = pct_test) %>% -->

<!--       ungroup() -->

<!--   }else{ -->

<!--     test_sim <-  -->

<!--       data_full_sim %>%  -->

<!--       slice_sample(prop = pct_test)  -->

<!--   } -->

<!--   # Put remaining data in train: -->

<!--   train_sim <- anti_join(data_full_sim, test_sim) -->

<!--   # Recreate our interaction counts for each item: -->

<!--   item_frequency_sim <-  -->

<!--     full_join( -->

<!--       train_sim %>% group_by(item) %>% count() %>% ungroup(), -->

<!--       test_sim  %>% group_by(item) %>% count() %>% ungroup(), -->

<!--       by = "item" -->

<!--     ) %>% -->

<!--     rename(count_train = n.x, count_test = n.y) %>% -->

<!--     replace_na(list(count_train = 0, count_test = 0)) -->

<!--   result[i] <- multinom.test(item_frequency_sim$count_train, item_frequency_sim$count_test)$pvalue  -->

<!-- } -->

<!-- mean(result <= alpha) -->

<!-- # size for stratified data: 0.2351 (using 10K reps). Not controlling size well.  -->

<!-- # size for non-stratified data: 0.0517 (using 10K reps) -->

<!-- ``` -->

<!-- Using 10K replications and $\alpha = 0.05$, the size of the test on the stratified sampled data is 0.2351 (not well controlled). However, the size of the test for the non-stratified sampled data is 0.0517, which is much better. In other words, the test is too sensitive for the stratified sampled dataset. -->

<!-- </details>  -->
