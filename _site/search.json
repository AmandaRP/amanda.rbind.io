[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2019-01-25-rstudio-conf-2019-recap/index.html",
    "href": "posts/2019-01-25-rstudio-conf-2019-recap/index.html",
    "title": "rstudio::conf recap",
    "section": "",
    "text": "The overview: I had the pleasure of attending rstudio::conf 2019 (in Austin, TX), a conference about all things R and RStudio. Over 1700 people attended the main conference, which consisted of three tracks (so my notes below cover only a third of the conference talks). There were an additional two days of tutorials and a two-day hackathon, which I did not attend. The 2020 conference will be in San Francisco Jan 27-30.\nMy thoughts: rstudio::conf (and also the yearly UseR! conference) is a great way to stay up to date on the latest and greatest R packages (there are currently 13K+ packages on CRAN) and to meet others excited about #rstats. They also gave the most useful swag that I’ve ever received at a conference: a binder full fo R package cheat sheets! … in addition to hex stickers and t-shirts. A big theme was using R in production; I found the talk about the use of R at T-mobile interesting. Additionally, the talks by Garrett Grolemund and Yihui Xie about rmarkdown/pagedown and the keynote by David Robinson, The Unreasonable Effectiveness of Public Work, motivated me to create this blog site!\nResources: Recorded videos of all talks are posted here. Additionally, links to slides can be found at here. And, check out the Twitter hashtag, #rstudioconf.\nRead more: Below are my notes from the talks that I attended. The post is a bit long, so I’ve divided it into the sections listed in the right margin."
  },
  {
    "objectID": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#viz",
    "href": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#viz",
    "title": "rstudio::conf recap",
    "section": "Visualization",
    "text": "Visualization\n3D mapping, plotting, and printing with rayshader, Tyler Morgan-Wall (Institute for Defense Analysis)\n\nRayshader: building realistic 3D maps from elevation data\nCan overlay images onto map. Animations possible. 3-D printing of maps.\nWatch the video for lots of cool examples.\nNext version will contain 3D ggplots\n\ngganimate Live Cookbook, Thomas Lin Pedersen (Software Engineer, RStudio)\n\ngganimate is an implementation of the grammar of animated graphics, extension to ggplot2.\nRewritten completely in the last year.\nNow available on CRAN!\nHow to use:\n\nTransitions: you want your data to change (transition_reveal: multiple enter/exit options )\nViews: you want your viewpoint to change\nShadows: you want the animation to have memory\n\nDocumentation: https://youtu.be/21ZWDrTukEs\nSee slides for examples."
  },
  {
    "objectID": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#modeling",
    "href": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#modeling",
    "title": "rstudio::conf recap",
    "section": "Modeling",
    "text": "Modeling\nIntroducing MLflow, Kevin Kuo (Software Engineer, RStudio)\n\nMLfow: an open source platform for the machine learning lifecycle.\nMotivation:\n\nKeeping track of what you did (trying different hyper-params, going back to a previous experiment, etc)\nReproducability\nLots of different modeling packages and ways to deploy them.\n\n\nSolving the model representation problem with broom, Alex Hayes (Tidyverse Intern, RStudio)\n\nModel representation problem: Need a standard way to represent models.\nbroom tools:\n\ntidy(): summarize info about fit components. Broom cleans up fit for us.\nglance(): report goodness of fit measures\naugment(): add info about observations to a dataset – not currently defined yet.\n\nPrint really pretty table of fit desc: tidy(fit) %>% kable2()\n\nCompare different models using purrr. (See code on slides)\nReal power: working with multiple models at once.\nResource: broom.tidyverse.org\nSee slides!!!\n\nParsnip - a tidy model interface (building models), Max Kuhn (Engineer, RStudio)\n\nWhy parsnip name? White carrot (a play on caret)\nMotivation for parsnip:\n\nDifferent packages have different modeling interfaces.\nFunctions run na.omit silently. Delete data.\n\nparsnip has a tidy interface\nSimilar to broom, nicely prints output.\nEx: set_engine(“glmnet”) , however, engine doesn’t necessarily have to be an R engine.\nNext steps:\n\nAdd more models and classes of models\nFormalize the API and tools for others to add parsnip models to their packages\n\n\nWhy Tensorflow Eager Execution matters, Sigrid Keydana\n\nTypically a static graph is generated. New way: eager execution\nSee blog articles."
  },
  {
    "objectID": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#other",
    "href": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#other",
    "title": "rstudio::conf recap",
    "section": "Other R Packages",
    "text": "Other R Packages\nMelt the clock: tidy time series analysis, Earo Wang (PhD student, Monash University)\n\nPackages:\n\ntsibble: to tidy and transform ts data\nmable: table of models\nfable: tidy forecasting (table of forecasts)\n\nCan use familiar broom functions on a mable: tidy, glance, augment. Then use geom_forecast (ggplot) to visualize.\n\nWorking with categorical data in R without losing your mind, Amelia McNamara (Assistant professor, University of St Thomas)\n\n“Practical Data Science for Statistics”: collection of papers. Avail on Github.\nIn R, categorical data is represented as factors.\nFactors are nice for 1) modeling, 2) reordering items in ggplot.\nPaper: Wrangling categorical data in R. Shows how factors behave unexpectedly.\nUse read_csv instead of read.csv! Better options for data formats.\nforcats: package to easily work with factors. Solves many of the issues that base R has with factors.\n\nSee her cheat sheet on RStudio website: syntax comparison.\nsummary() is your friend (along with testthat) to make sure data isn’t changing unexpectedly.\n\nBuilding an A/B testing analytics system with R and Shiny, Emily Robinson (Data Scientist, DataCamp)\n\nfunneljoin - R package for common A/B testing analyses."
  },
  {
    "objectID": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#doc",
    "href": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#doc",
    "title": "rstudio::conf recap",
    "section": "Documentation",
    "text": "Documentation\nR Markdown: The Bigger Picture, Garrett Grolemund (Data scientist and Educator, RStudio)\n\nCo-other of R Markdown the Definitive Guide\nReplication crisis example: Estimate: 75%-90% of preclinical results cannot be reproduced, which costs $28 billion per year in the US.\nRmarkdown provides a map for other scientists to reproduce your results.\n\nPagedown: Creating Beautiful PDFs with R Markdown, Yihui Xie (Software Engineer, RStudio) – collaborator: Romain Lesur\n\nStatus: package is still experimental (obtain from github)\npagedeown creates paged documents (e.g. PDF) from web pages.\nChrome or chromium is recommended browser to view.\nCan also use to create:\n\nbusiness cards.\nResume (e.g. https://pagedown.rbind.io/html-resume)\nPosters\nLetters\nBooks\n\nSlides: http://bit.ly/pagedown\n\nIntroducing the the gt Package, Rich Iannone (Software Engineer, RStudio) @riannone\n\ngt lets you build pretty display tables with easy-to-use functions for HTML, LaTeX and RTF.\nCurrently on Github (not yet on CRAN)\ntibble %>% gt() or dataframe %>% gt()\nSee slides for an example of sending an email (with a table) from RStudio."
  },
  {
    "objectID": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#programming",
    "href": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#programming",
    "title": "rstudio::conf recap",
    "section": "General Programming",
    "text": "General Programming\nDemocratizing R with Plumber APIs, James Blair (Solutions Engineers, RStudio)\n\nplumber: Easily create API endpoints around R functions.\nInteracts with OPENAPI (Swagger)\nUse case: Use plumber to expose R models via an API\nMost recent updates avail in github repo\nResources:\n\nPlumber: www.rplumber.io\nopenAPI: https://swagger.io/docs/specification/about\nGithub: github.com/sol-eng/plumber-model\n\n\nVctrs: tools for making size and type consistent functions, Hadley Wickham (Chief Scientist, RStudio) @hadleywickham\n\nvctrs: a new package that provides tools to ensure that functions behave consistently with respect to inputs of varying length and type.\nBase R: When combining objects of different classes using c(), the following order of precidence occurs: Logical -> integer -> double -> character (combine 2 types in an atomic vector and you get the type on the right)\nvctrs is more strict than an atomic vector in base R.\nWill be integrated into tidyverse packages (behind the scenes) during 2019.\n\nTidy eval context, Jenny Bryan (Software Engineer, RStudio)\n\nTidy evaluation is a toolkit for metaprogramming (code that writes/mutates code) in R.\nTidyverse itself does alot of metaprogramming (behind the scenes).\nSimilar to nonstandard evaluation or unquoted variable names.\nPackage: rlang (rlang.r-lib.org) is a developer facing package. Most people will not need to use.\nMight need enquo or !! if passing variables to tidyverse function.\nMy perspective: This is more in the low level weeds than I need.\n\nBox plots: a case study in debugging and perseverance, Kara Woo (Research Scientist, Sage Bionetworks)\n\nTalked through an example of a difficult debugging problem (for the ggplot2 package). * Advice:\n\nUse a reprex (minimal reproducible example) to determine/reproduce problem.\nUse debug function. (Let’s you step through function.)\n\nHow do you know when you are done? Don’t let perfect be the enemy of good."
  },
  {
    "objectID": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#interop",
    "href": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#interop",
    "title": "rstudio::conf recap",
    "section": "Interoperability with other languages",
    "text": "Interoperability with other languages\nScaling R with Spark, Javier Luraschi (Software Engineer, RStudio)\n\nsparklyr package avail on cran (also need spark on machine)\nConnecting to Spark: “Connections” pain in the upper right tab. -> new connection -> spark. Click the “Connect from” dropdown to see interaction options (e.g.: command line, R notebook)\nTo start: sc <- spark_connect(master=”local”)\nCan use dplyr or sql code for data manipulation/query\nNew features:\n\nIntroduced pipelines. Can export to be used by ppl working in other languages.\nSpark structured streams: process streaming data with R dataframes. Apply usual dplyr transformations.\n\nCurrently working on:\n\nSupport for Apache arrow. Install R arrow package from github.\nXgboost models in spark\n\nResources:\n\nDocs: spark.rstudio.com\nBlog: blog.rstudio.com/tags/sparklyr\nStackoverflow: stackoverflow.com/tags/skparklyr\n\n\nUrsa Labs and Apache Arrow in 2019: Infrastructure for next-gen data science toolbox, Wes McKinney (Ursa Labs, Python Pandas Creator)\n\nhttps://ursalabs.org\nR and Python programmers solving many of the same problems.\nVision: multicore algorithms, lazy eval, sophisticated memory management, interoperable memory models, interchangeable between languages.\nArrow: founded in 2016. Language agnostic, open standard in-memory format for data frames. Tools (languages) share arrow memory. Goals: faster access to data, move data efficiently, compute analytics fast.\nArrow for R: Building bindings for R.\nPlans to improve the feather package. The feather file format is readable by both R and Python."
  },
  {
    "objectID": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#production",
    "href": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#production",
    "title": "rstudio::conf recap",
    "section": "R in Production",
    "text": "R in Production\nAPI Development with R and Tensorflow at T-Mobile, Heather Nolis & Jacqueline Nolis (Machine Learning Engineer & Data Scientist, nolisllc) {#tmobile}\n\nT-mobile has 70 million customers (+ additional from merger with Sprint)\nNLP models to classify incoming customer message (including account info) to give live agent a heads up about the likely topic. (They used a CNN via the keras R package.)\nTheir workflow steps: First Rmarkdown for exploratory data analysis. Second show model to business people using a shiny demo to get them interested. Success: Given millions of $ to put it into production.\nWas told: “If you want to do ML in production, you have to use Python.” Idea: Treat R like a real programming language (because it is one)! Didn’t want to re-write everything. Steps:\n\nTurn R into an API using plumber\nUse docker images (rocker)\nPlumber doesn’t support https. Used an appache2 server to reroute.\nContainer was too big. Swapped miniconda for anaconda. Removed RStudio & some unnecessary Linux, R, and python packages.\n\nThis model is now deployed and used in production at T-mobile.\nTheir docker container is now open source: https://github.com/tmobile/r-tensorflow-api\nSlides: nollisllc.com/rstudio19\n\nKeynote: Tareef Kawaf (President, RStudio),\n\nRStudio maintains 170 R packages\nMaking the life of an SA better: RStudio Server Pro, RStudio package manager.\nRstudio connect to publish Shiny applications, R Markdown reports, Plumber APIs, dashboards, plots, and more.\n\nKeynote: Shiny in Production, Joe Cheng (CTO of RStudio)\n\nCloud.r-project.org: cran mirror managed by RStudio. Download logs available (for that mirror).\nQ: Can shiny be used for production. Ans: Yes! (It’s quite easy.) Challenges: Cultural (shiny apps usually developed by R users, who aren’t necessarily SWEs). Organizational (IT and management can be skeptical). Technical (shiny lowers bar for creating web app… but not easy to automated testing, load testing, profiling, and deployment … but RStudio has worked on making this better).\nPet peeve: “R is not a real programming language.”\nNew tools for shiny in production:\n\nRStudio Connect - Shiny serving with push-button deployment\nShinytest (now on cran) - automated ui testing for shiny\nShinyloadtest - load testing for shiny\nprofvis - profiler for R\nPlot caching - in Shiny 1.2. Speed up plots. Use if plots are 1) slow, 2) significant fraction of total amt of time the app spending thinking, 3) most users likely to request the same few plots.\nAsync - last resort for slow portions.\n\nUse shinyloadtest to see if it’s fast enough (generates large amounts of traffic). If not, use profvis to see what’s making it slow. Then optimize: move work out of shiny (very often). Make code faster (very often), using caching (sometimes), use async (occasionally). Repeat.\nDeploy apps to either RStudioConnect or Shiny Server.\nReading from feather files is faster than csv.\nResources:\n\nBook: Shiny in Production (in progress)\nSlide deck\n\n\nR in Production Mark Sellers (Data Engineer, Mango Solutions)\n\nAuthored: Field Guide to the R Ecosystem\nBiggest challenge to using R in production: cultural, not technical."
  },
  {
    "objectID": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#rstudio",
    "href": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#rstudio",
    "title": "rstudio::conf recap",
    "section": "RStudio Updates",
    "text": "RStudio Updates\nNew Language Features in RStudio 1.2, Jonathan McPherson (Engineer, RStudio)\n\nGoals: more comprehensive R project workbench. Embrace other languages commonly used in R data science projects, reduce context switching.\nNon-goals: Become a general purpose IDE. Lose focus on R.\nDemo-ed the following languages within one R notebook.\n\nSQL: Connections tab -> new connection -> ODBC. Write SQL in RStudio (as a code chunk in R notebook, or in a SQL file).\nPython: reticulate package\nD3: r2d4 function\n\nBackground scripts: “source as local job” button to run job in the background. Can do other things in RStudio while it’s running. Multiple jobs can be run at once. Useful for long running computations.\nMake powerpoint presentation with RMarkdown: New file -> presentation -> PowerPoint\nStable release of 1.2 this spring."
  },
  {
    "objectID": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#education",
    "href": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#education",
    "title": "rstudio::conf recap",
    "section": "Education",
    "text": "Education\nR4DS online learning community, Jesse Mostipak (Data Scientist, Teaching Trust)\n\nStarted the R for Data Science (R4DS) slack group.\nRules: You will be kicked out of the group for being not-nice.\nwww.Rfordatasci.com\n@R4DScommunity\nTidyTuesday\n\nKeynote: Explicit Direct Instruction in Programming Education, Felienne (Associate Prof, Leiden University) @felienne\n\nTopic: How to teach programming?\nExamples of direct instruction:\n\nVocalize code snippets (when teaching kids)\nExplanation and practice works best (as opposed to explore). Skill -> motivation.\n\nInteresting & entertaining talk for people interested in the teaching of coding.\n\nThe next million R users Carl Howe (Director of Education, RStudio)\n\nSurvey: See rstd.io/learning-r-survey. 3300 responses. (warning: sampling bias may be present)\nWho uses R? 110 countries responded.\nMost have advanced degrees.\n⅔ of R users use tidyverse today.\n15% of R users have no one else in their work group that knows R :(\nResources:\n\nRStudio teacher certification available.\nFree academic licensing for RStudio pro tools. Just send course syllabus from a certified academic institution. Research (instead of teaching) gets a 50% discount.\nData Science in a box: https://datasciencebox.org\nLots of free online books: R for Data Science, Advanced R, Blogdown, Hands on programming with R, Geocomputation in R.\nrstudio.cloud (free) primers for self learning.\n\n\nIntroductory Statistics with R: Easing the transition to software for beginning students, Kelly Nicole Bodwin (Faculty, Cal Poly) @kellybodwin\n\nThey use pre-lab exercises built in Shiny to let the students practice the statistics before having to code in R. Demo of a pre-lab exercise shown in talk.\nlearnr package was used to build shiny apps.\nGithub repo with exercises: github.com/kbodwin/Introductory-Statistics-Labs\nDemos:\n\nhttps://kbodwin.shinyapps.io/Lab_Exercise_CatVars2\nhttps://kbodwin.shinyapps.io/Lab_Exercise_tDist\nhttps://kbodwin.shinyapps.io/t_tests2\n\n\nTeaching data science with puzzles, Irene Steves (former intern, RStudio) @isteves\n\nPuzzle name: Tidies of March\nR package: tidiesofmarch\nSlides and code available at: bit.ly/ds-puzzles\n\nCatching the R wave how R and RStudio are revolutionizing statistics education in community colleges (and beyond), Mary Rudis (Math Dept Chair, Great Bay Community College)\n\nShiny apps for teaching stats:\n\nhttps://statistics.calpoly.edu/shiny\nhttps://shinyapps.science.psu.edu\n\nGithub: github.com/mrshrbrmstr/RStudioConf2019 (includes lesson plans)"
  },
  {
    "objectID": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#ds",
    "href": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#ds",
    "title": "rstudio::conf recap",
    "section": "Data Science",
    "text": "Data Science\nUsing Data Effectively: Beyond Art and Science, Hilary Parker (Data Scientist, Stitch Fix)\n\nStitch Fix has very little data on both the person and the clothing item… traditional matrix factorization (collaborative filtering) does not work well. Soln: They added the “style shuffle” where customer flips through a bunch of clothes and says yes/no to whether they would wear it. This enabled traditional matrix factorization.\nR magik package for image processing.\n\nData Science as a Team Sport, Angela Bassa (Director of Data Science, iRobot) @angebassa\n\nJust because a data scientist can do everything, doesn’t mean they should.\nData engineer != data scientist\nWhen to grow? When you’d like to grow scope & maturity. Adding people will slow things down (additional complexity) unless systems are improved.\nWhen adding people: add specialization (people have different roles in the DS pipeline), add process (documentation, authentication, governance, data provenance, automation: testing, deployment), add resilience (hiring, methodical on-boarding, culture, diversity & inclusion)\nIdea: Documentation party … offsite … have pizza.\nHire both experts and interns (the latter question what experts have forgotten to question)\nWhere should DS team live? Embedded or centralized? Her answer: It doesn’t matter. The importance is how they interact and communicate. But, remember teams don’t scale.\nIn her experience the best model she’s found is 5-10 data scientists (of differing background/expertise) + ~3 data engineers on a team. If more then split into multiple teams.\n\nKeynote: The Unreasonable Effectiveness of Public Work, David Robinson (Chief Data Scientist, Data Camp) {#dr}\n\nWork shared publically is way more useful than work local on your computer.\nEffective ways to share: blog, tweet, contribute to open source, give talks, record screencasts, write a book.\nWhen you’ve given the same in-person advice 3 times, write a blog post.\nIf you start a data-related blog, tweet link to @drob and he’ll tweet about your first post.\nWhat to blog about?\n\nAny paper you’re written. (more exposure)\nCurrent events\nTidyTuesday\nTeach a concept\nBlog about something that you’re learning.\nExamples: #datablog hashtag on Twitter\n\nTwitter:\n\nOne for each blog post.\nPromote others’ work\nTidytuesday evaluation.\nWhat you’ve learned at a conference\nNot a great way to document knowledge for long term. Blog better for that.\nFrom:username to search within one person’s tweets.\n\nContribute to open source:\n\nSee Ten Steps to Becoming a Tidyverse Contributor by Nic Crane\nContribute to a beginner-friendly issue\nWrite R packages (See R Packages book)\n\nGive talks:\n\nSee Giving your first data science talk by Emily Robinson\nIntrovert tip: It makes networking at conferences easier.\nBe sure to publish your slides.\n\nRecord screencasts:\n\nLimitation: you need to be capable and confident enough to improvise.\nRachel Tatman does live coding on Twitch.\n\nWrite a book:\n\nYou need a good amount to say and some practice saying it.\nR bookdown package.\n\nSlides: bit.ly/drob-rstudio-2019\n\nPanel Discussion: Growth and Data Science: Individuals, leaders, organizations and responsibilities, Speakers: Hilary Parker (Data Scientist, Stitch Fix), Karthik Ram (Data Science Fellow, UC Berkeley), Angela Bassa (Director of Data Science, iRobot), Tracy Teal (Exec Dir, Carpentries), Eduardo (Data Science Leader, Instagram)\n\nSee Hilary’s Not so standard data science podcast\nWhat is the most important thing for success as a data scientist:\n\nFlexibility in tooling (no space for a tooling purist)… but know at least one language fluently. Softer skills (e.g. empathy, understanding user/reader)\nIf an organization is giving you a hard time about your lack of knowledge as a junior data scientist, consider a different company. Don’t shy away from saying “I don’t know.” Her interview guide has impossible questions to answer… she’s looking for humility to say “I don’t know.”\n\nHow to choose to become a DS lead vs staying technical:\n\nData scientists aren’t necessarily given opportunity to learn/grow leadership skills. Need to be more strategic to grow better leaders. Some DS’s stumble into a management/leadership position.\nAngela didn’t want to become a manager and wanted to stay in trenches at first, but now loves it (took the position b/c didn’t want to let her management down).. “Management is a skill that is learnable.” If you feel accomplishment from management, great rock it. If you don’t, then no worries don’t be a manger.\nIt takes personal growth to stop self-depricating yourself as a manager: “I just read email and schedule meetings.”\nFear: Skills will atrophy. Remember “loops are still loops.” Opportunity to develop additional skills. Redefine what success means: How can I enable people to do their best work?\n\nIn a growing organization that you’ve been at, what is the most important lesson to do/not do?\n\nDon’t hire DS if you don’t have data.\nAdapt as the business grows.\nInvest in systems that let people work effectively together\nDon’t over hire. No clear success criteria.\nFor machine learning projects, useful to have a PM that understand machine learning. There types of people can be rare.\n\nResponsibilities as Data scientists:\n\nData are artifacts…not ground truth.\nOwn your mistakes.\nMake work reproducible.\nSee datasciencemanifesto.org\n\nWhat is the most effective way that program management and DS works together:\n\nProduct has a road map. Make sure that DS also has a road map and that they are correlated.\n\nWhat are common ways that data scientists fail?\n\nBy not saying “no” enough then only providing a cursery analysis that has little impact.\nAssuming the PM knows enough to ask the question in the best way.\nCaring more about the stats method/model than solving the problem for the business."
  },
  {
    "objectID": "posts/2019-08-02-twitter-likes/index.html",
    "href": "posts/2019-08-02-twitter-likes/index.html",
    "title": "Shiny App: Twitter Likes",
    "section": "",
    "text": "rtweet Package\n\n\n\n\n\n{{< tweet dataandme 1109764129005420544 >}}\n{{< tweet juliesquid 1557485909486059523 >}}\nTo grab the tweets, there is a little setup with Twitter that is necessary. The setup is described on the rtweet website.\n\n\n\nDuring setup, I ran into a bit of an issue authenticating my Twitter app. I’m working in RStudio on an AWS instance (via a Chromebook), which has a known issue. The issue and its work-around are described by the package author here. After that speed bump was crossed, the rtweet package was extremely easy to use!\n\n\nMy Twitter Likes\nTo get a Twitter user’s favorites (aka likes), use the get_favorites function. Below I’m going to grab my most recent five favorites:\n\nfavorites <- get_favorites(\"DrAmandaRP\", n = 5, token = token)\n\nThe resulting tibble has 91 columns!\nFor fun, here is a wordcloud composed of all of my Twitter likes (created using the tidytext and wordcloud2 packages):\n\nlibrary(wordcloud2)\nlibrary(tidytext)\nfavorites <- get_favorites(\"DrAmandaRP\", n = 1000, token = token)\nfavorites %>% \n  select(text) %>%\n  unnest_tokens(word, text) %>%\n  count(word, sort = TRUE) %>%\n  anti_join(stop_words) %>%\n  filter(!word %in% c(\"t.co\", \"https\", \"http\")) %>%\n  wordcloud2()\n\n\n\n\n\n\n\n\nshiny app for searching\nI zeroed in on the following fields: status_id, created_at, text, hashtags, name, screen_name. I put them in a table in a shiny app for easy browsing. Check it out here! It’s nothing fancy, but I think it’s going to come in handy.\nIf you’d like to make your own app, my code is available on GitHub.\nIt would also be nice to add functionality to search Twitter bookmarks, but apparently at the time of this post, reading bookmarks isn’t yet available in the Twitter API. I’m keeping an eye on this rtweet issue for updates."
  },
  {
    "objectID": "posts/2019-03-10-rstats-bingo/index.html",
    "href": "posts/2019-03-10-rstats-bingo/index.html",
    "title": "Statistics Bingo",
    "section": "",
    "text": "I just wrapped up another round of teaching stats. I co-teach two courses per year as an adjunct instructor: one class that covers intro stats and the other that hits higher level concepts. Both courses move at a high pace; they’re intended to be a survey course for professionals with a STEM background. Some students have a math background, but have never taken a statistics course. Other students have had statistics a long time ago in their undergraduate studies, but want to brush up on the material.\nIn both classes, by the time we get to the end, the students appreciate a light hearted activity. I found bingo to be the perfect activity to reveiew material and have a little fun. Jenny Bryan and Dean Attali have an R Shiny app for creating such bingo games. Their site has a handful of pre-populated game themes such as “boring meeting” and “bad data”. Or you can choose to create your own by pasting a list of words.\nI created my own bingo questions that I ask during the game and answers that are written on the bingo cards. (Tip: Don’t forget to bring the associated list of questions to class like I did!) I’ve pasted the questions and answers below that I used for each class.\nLastly, don’t forget the sugar! M&M’s make great bingo chips. And, cookies decorated with statsistics are a crowd pleaser."
  },
  {
    "objectID": "posts/2019-03-10-rstats-bingo/index.html#bingo-questions-for-probability-statistics-i",
    "href": "posts/2019-03-10-rstats-bingo/index.html#bingo-questions-for-probability-statistics-i",
    "title": "Statistics Bingo",
    "section": "Bingo Questions for Probability & Statistics I:",
    "text": "Bingo Questions for Probability & Statistics I:\n\n\n\n\n\n\n\nQuestions\nAnswers\n\n\n\n\n1. The R function used to find the area under the normal curve\npnorm\n\n\n2. For a discrete random variable: \\(E(X) = \\sum_i^n X_i\\) * ____\n\\(P(X)\\)\n\n\n3. Step 5 of the hypothesis testing method is to provide a conclusion in the _____.\ncontext of the problem\n\n\n4. Square root of the variance\nstandard deviation\n\n\n5. Mathematical function used to count number of possibilities: without replacement, order doesn’t matter.\nchoose\n\n\n6. A ___ sample results when the population is divided into at least two different subpopulations based on some characteristic, and a sample is drawn from each subpopulation.\nstratified\n\n\n7. The Central Limit Theorem says that the distribution of the mean converges to this distribution as \\(n\\) goes to infinity (variance must be defined).\nnormal\n\n\n8. The ____ distribution approximates the Binomial when \\(n\\) is large and \\(p\\) is small. In this case we set \\(\\lambda=np\\).\nPoisson\n\n\n9. \\(P(A \\cup B) = P(A) + P(B) -\\) ___ ?\n\\(P(A \\cap B)\\)\n\n\n10. Discrete distribution that models the number of successes in \\(n\\) Bernoulli trials, where \\(p\\) is the probability of success.\nBinomial\n\n\n11. The test statistic in the Test of Independence and The Goodness of Fit has a ___ distribution.\nChi-Square\n\n\n12. This distribution has PMF \\(P(X=k) = p^k(1-p)^{1-k}\\)\nBernoulli\n\n\n13. Used to control the type I error of a hypothesis test\nsignificance level (\\(\\alpha\\))\n\n\n14. In hypothesis testing, we reject the null hypothesis if the test statistic falls in the ____ region.\ncritical\n\n\n15. Bayes Rule: \\(P(A \\text{ given } B) =\\) ____ * \\(\\frac{P(A)}{P(B)}\\)\n\\(P(B \\text{ given } A)\\)\n\n\n16. The area under the entire PDF curve\n1\n\n\n17. The continuous distribution that looks like a bell curve (except that it has fat tails), converges to normal as \\(n\\) increases, and is used when modeling data with small sample size.\nStudent’s \\(t\\)\n\n\n18. The maximum minus the minimum\nrange\n\n\n19. The most frequent observation in a data sample\nmode\n\n\n20. The function \\(F(X) = P(X \\le x)\\). You could describe it as a running total of the probability density function.\nCDF\n\n\n21. A ___ provides a range for which we are \\((1-\\alpha)\\)% confident contains the true value of the population parameter. In class we considered such ranges for the true mean, \\(\\mu\\), and for proportions.\nconfidence interval\n\n\n22. The ___ error occurs when the null hypothesis is true, but we reject it in favor of the alternative hypothesis.\nType I\n\n\n23. The ___ of a hypothesis test is the probability that we reject the null hypothesis when it is in fact false.\npower\n\n\n24. \\(1 - P(A) =\\) the probability of the ___ of \\(A\\).\ncompliment"
  },
  {
    "objectID": "posts/2019-03-10-rstats-bingo/index.html#bingo-questions-for-probability-statistics-ii",
    "href": "posts/2019-03-10-rstats-bingo/index.html#bingo-questions-for-probability-statistics-ii",
    "title": "Statistics Bingo",
    "section": "Bingo Questions for Probability & Statistics II:",
    "text": "Bingo Questions for Probability & Statistics II:\n\n\n\n\n\n\n\nQuestions\nAnswers\n\n\n\n\n1. Let {\\(X_i\\)} for \\(i=1..n\\) partition the sample space. Bayes rule says that \\(f(X_i \\text{ given } Y) = \\frac{f(Y \\text{ given } X_i)f(X_i)}{\\sum_{i=1}^{n}f(Y \\text{ given } X_i)f(X_i)}\\). The ___ law is used in denominator when marginal, \\(f(Y)\\), is unknown.\nLaw of Total Probability\n\n\n2. Maximization of the log likelihood function (take derivative and set equal to 0)\nMLE\n\n\n3. Probability density function for 2 or more variables\nJoint PDF\n\n\n4. Function used to count number of possibilities: without replacement, order doesn’t matter\nchoose\n\n\n5. Result of integrating \\(f(X,Y)\\) with respect to \\(Y\\), over the support of \\(Y\\)\nmarginal of \\(X\\)\n\n\n6. An alternative to frequentist statistical analysis\nBayesian statistics\n\n\n7. CLT says that the distribution of the mean converges to this distribution as \\(n\\) goes to infinity (variance must be defined)\nNormal\n\n\n8. \\(P(A \\cap B) = P(A \\text{ given } B)\\) * ____\n\\(P(B)\\)\n\n\n9. Discrete distribution that models number of trials until 1 success\nGeometric (special case of negative binomial)\n\n\n10. Continuous distribution use to model time until events occur\nGamma\n\n\n11. True or false: \\(Cov(X,Y)=0\\) implies \\(X\\) and \\(Y\\) are independent. (TRUE if \\(X\\) and \\(Y\\) are jointly normally distributed. Independence implies \\(cov=0\\) ).\nFALSE\n\n\n12. \\(E(XY) = E(X)E(Y)\\) when X and Y are independent\nTRUE\n\n\n13. \\(Var(X) = E(X^2) -\\) ____\n\\(E(X)^2\\)\n\n\n14. \\(V(aX+b) =\\) ___\n\\(a^2V(X)\\)\n\n\n15. Name of this function: \\(E(X^{tx})\\) (Note: obtain \\(k^{th}\\) moment by taking \\(k^{th}\\) derivative and evaluating at \\(t\\)=0)\nMoment Generating Function\n\n\n16. This distribution has PMF \\(P(X=k)= p^k(1-p)^{1-k}\\)\nBernoulli\n\n\n17. Used in sampling from strata to ensure that the proportion of observations in the sample mimic the population proportions.\nProportional allocation\n\n\n18. Used to control the type I error of a hypothesis test\nSignificance level (\\(\\alpha\\))\n\n\n19. In Bayesian statistics, the population parameters are considered to be ____.\nrandom variables\n\n\n20. \\(P(A \\text{ given } B) =\\) ____ * \\(\\frac{P(A)}{P(B)}\\)\n\\(P(B \\text{ given } A)\\)\n\n\n21. \\(P(A \\cup B) = P(A) + P(B)\\) - ______\n\\(P(A \\cap B)\\)\n\n\n22. The area under the entire PDF curve\n1\n\n\n23. The derivative of the CDF with respect to the random variable (for a single random variable)\nPDF\n\n\n24. The second non-central moment\nVariance\n\n\n25. If \\(E(\\hat{\\theta}) = \\theta\\) we say \\(\\hat{\\theta}\\) is ___.\nUnbiased"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Amanda's Data Science Blog",
    "section": "",
    "text": "rstats\n\n\nshiny\n\n\ntwitter\n\n\n\nAnalysis of my Twitter likes via a wordcloud and a shiny app.\n\n\n\nAmanda Peterson\n\n\nAug 2, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\nsentiment\n\n\nbible\n\n\n\n\n\n\n\nAmandaRP\n\n\nJun 22, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\nrstats\n\n\n\nAn R package for two sample hypothesis testing of high dimensional discrete data\n\n\n\nAmanda Peterson\n\n\nMay 24, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\neducation\n\n\n\nBingo for a statistics course\n\n\n\nAmanda Peterson\n\n\nMar 10, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\nconference\n\n\n\nMy notes from 2019 rstudio::conf\n\n\n\nAmanda Peterson\n\n\nJan 26, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2019-05-24-new-r-package-hddtest/index.html",
    "href": "posts/2019-05-24-new-r-package-hddtest/index.html",
    "title": "Introducing the hddtest R package",
    "section": "",
    "text": "I’ll illustrate the use of one of its functions, the multinomial neighborhood test, by using a dataset included in hddtest called twoNewsGroups. It contains document term frequency matrices for two of the 20 newgroups. Each matrix has dimension 594 by 16214. The \\((i,j)\\) entry of each matrix is the count (term frequency) of the \\(j^{th}\\) word in the \\(i^{th}\\) document. The first matrix in the list contains 594 sampled documents from the rec.sport.baseball newsgroup. The second contains 594 sampled documents from the sci.med newsgroup.\nWe might like to compare two sets of documents to determine whether or not they come from the same newsgroup. To do this, we’ll perform a neighborhood test.\nWhat is a neighborhood test useful for? In testing the equality of parameters from two populations, it frequently happens that the null hypothesis is rejected even though the estimates of effect sizes are close to each other; however, these differences are so small that parameters may not be considered to be different in practice. Another issue is that although the use of p-values is a common measure to draw a conclusion about the population, one may be interested in the measure of indifference or inhomogeneity.\nWe’ll start by sampling two sets of 200 documents from the sci.med newsgroup. We’ll use this to simulate the null hypothesis being TRUE.\n\nlibrary(hddtest)\ndata(twoNewsGroups)\n\nnum_docs <- 200\nrow_ids <- 1:nrow(twoNewsGroups$sci.med)\ngroup_1 <- sample(row_ids, num_docs)\ngroup_2 <- sample(row_ids[-group_1], num_docs)\n\nNext for each of the two groups, sum the 200 term frequency vectors together. They will be the two multinomial vectors that we test. We’ll store the result in a list called, vecs2Test.\n\nvecs2Test <- list(NA, 2)\nvecs2Test[[1]] <- twoNewsGroups$sci.med[group_1, ] |> \n  colSums() |> \n  matrix(nrow = 1)\nvecs2Test[[2]] <- twoNewsGroups$sci.med[group_2, ] |>\n  colSums() |> \n  matrix(nrow = 1)\n\nNow test the null hypothesis that the associated multinomial probability vectors are within some neighborhood, delta, of each other (against the alternative that they are not). We can then use this result to infer whether the sets of documents are likely from the same newsgroup.\n\nvecs2Test |> multinom.neighborhood.test(delta = 60)\n#> $statistic\n#> [1] 7.811123\n#> \n#> $pvalue_delta\n#>      [,1]\n#> [1,]    1\n\nHere we fail to reject the null hypothesis using a delta of 60. How to choose the appropriate delta? The answer may come from subject matter expertise about the problem domain. Or you can run a simulation to gain insight. Below we define a simulation function.\n\nsimulation <- function(data, null_hyp, delta, reps = 30, num_docs = c(200, 200)){\n  \n   vecs2Test <- list(matrix(NA, reps, ncol(data[[1]])), matrix(NA, reps, ncol(data[[1]])))\n   \n   for(i in 1:reps){\n     if(null_hyp){\n       # Sample two sets of num_docs from the SAME set of documents     \n       row_ids <- 1:nrow(data[[2]])\n       group_1 <- sample(row_ids, num_docs[1])\n       group_2 <- sample(row_ids[-group_1], num_docs[2])\n       vecs2Test[[1]][i, ] <- data[[2]][group_1, ] |> colSums()\n       vecs2Test[[2]][i, ] <- data[[2]][group_2, ] |> colSums()\n       \n     }else{\n       # Sample num_docs from each of the two DIFFERENT sets of documents\n       vecs2Test[[1]][i, ] <- data[[1]][sample(1:nrow(data[[1]]), num_docs[1]), ] |>\n                                      colSums()\n       vecs2Test[[2]][i,] <- data[[2]][sample(1:nrow(data[[2]]), num_docs[2]), ] |>\n                                      colSums()\n     }\n   }\n   \n   #Perform the test:\n   result <- vecs2Test |> multinom.neighborhood.test(delta = delta)\n   \n} #end simulation function\n\nNow run the simulation for varying values of delta (in the range 1 to 160) testing both the null and alternative hypotheses for 30 replications each. For the null hypothesis simulation, we sample documents from the same newsgroup. For the simulation of the alternative hypotheis, we sample documents from the two different newsgroups. The resulting plot shows one curve for each the 60 simulations (which compute the p-value at each value of delta.)\n\ndelta <- 1:160\np.delta.null <- simulation(data = twoNewsGroups, null_hyp = TRUE, delta = delta)$pvalue_delta\np.delta.alt  <- simulation(data = twoNewsGroups, null_hyp = FALSE, delta = delta)$pvalue_delta\n\n\n\n\n\n\nNotice in the plot above that for delta in the range of about 40 to 100, the p-value is large when the the two sets of documents come from the same newsgroup (shown in blue) and small when the two sets of documents come from different newsgroups (shown in red). So, our previous choice of delta=60 seems reasonable for making the correct conclusion.\nThe methods for this package were developed in collaboration with my UMBC PhD thesis advisor, Dr Junyong Park, and published in 1 and 2. Full details about the statistics used and their distributions are documented in these papers. To see the full list of functions available in hddtest, see the GitHub README. Thanks for reading!"
  },
  {
    "objectID": "posts/2019-05-18-text-analysis-of-psalms/index.html",
    "href": "posts/2019-05-18-text-analysis-of-psalms/index.html",
    "title": "Sentiment Analysis of Psalms",
    "section": "",
    "text": "get_chapter <- function(chapter){ #Pull text and do a bit of cleaning:\n  \n  url <- str_c(\"https://www.biblegateway.com/passage/?search=Psalms+\", \n               chapter, \n               \"&version=GNT\")\n  \n  html <- read_html(url)\n  \n  text <- html %>% \n    html_node(\".passage-wrap\") %>% \n    html_text() %>%\n    str_extract(\"\\\\(GNT.{1,}\") %>%\n    str_replace(\"\\\\(GNT\\\\)\",\"\") %>%\n    str_extract(\"\\\\d.{1,}\") %>%\n    str_replace_all(\"\\\\d{1,}\", \"\") %>%\n    str_replace_all(\"\\\\[\\\\w{1}\\\\]\",\"\")\n}\n\n#Use the function defined above to read all 150 chapters.\nchapters <- map_chr(1:150, get_chapter) %>% \n  tibble::enframe(name = \"ch_num\", value = \"text\") \n\n\nPositive and Negative Word Lexicon\nNext, we’ll use the tidytext package to tokenize (i.e. split the text by words) and join our data with a dictionary of sentiment words. For more information about text analysis using tidytext, see Text Mining with R: A Tidy Approach by Julia Silge and David Robinson. tidytext comes with sentiment dictionaries, but I’m going to use the Jockers & Rinker sentiment dictionary from the lexicon package to better compare with a follow-on analysis using the sentimentr package. This dictionary contains positive and negative words and an associated sentiment score in the range [-1, 1]. A value of 1 is the most positive, 0 is neutral, and negative 1 is most negative.\nLet’s first look at a comparison word cloud to compare the frequency of the positive and negative words in the book.\n\n#Tokenize and join with sentiment lexicon:\npsalms_sentiment_jockers_rinker <- chapters %>% \n  unnest_tokens(word, text) %>%  #tokenize by words\n  #anti_join(stop_words) %>%\n  left_join(lexicon::hash_sentiment_jockers_rinker, \n             by = c(\"word\" = \"x\"), \n             drop = FALSE) %>%\n  mutate(y = replace_na(y, 0)) \n\n#Draw the comparison cloud:\npsalms_sentiment_jockers_rinker %>%\n  filter(abs(y) > 0) %>%\n  mutate(pos = y>0, neg = y<0) %>%\n  select(-ch_num, -y) %>%\n  group_by(word, pos, neg) %>%\n  summarize(cnt = n()) %>%\n  mutate(Positive = cnt * pos, Negative = cnt * neg) %>%\n  ungroup %>%\n  select(word, Positive, Negative) %>%\n  as.data.frame() %>%\n  column_to_rownames(\"word\") %>%\n  comparison.cloud(title.colors = \"black\")\n\n\n\n\n\n\n\n\nSentiment Analysis by Chapter\nThere is some flexibility in the method that we may choose to compute the sentiment. We could sum the sentiment scores for the words in each chapter, which introduces a relationship between sentiment score and chapter length. Or we could compute the average sentiment over the words in the chapter, either choosing to ignore or include neutral words (i.e. words with score of 0). The inclusion of neutral words in the calculation of the average would dampen the overall sentiment score of the chapter. I think the choice depends on what makes the most sense for each application.\nIn the interest of better comparing this first calculation to a second calculation using the sentimentr package, I’m going use the mean chapter sentiment (including neutral words). That is, the sentiment for the \\(j^{th}\\) chapter is\n\\[S_j =  \\frac{1}{n_j} \\sum_{i=1}^{n_j} s_{ij}\\]\nwhere \\(n_j\\) is the word count for chapter \\(j\\) and \\(s_{ij}\\) is the sentiment score for the \\(i^{th}\\) word in the \\(j^{th}\\) chapter.\n\n#Compute sentiment:\npsalms_sentiment_jockers_rinker %<>%\n  group_by(ch_num) %>%\n  summarize(avg_sentiment = mean(y))\n\n\n\n\n\n\n\n\n\nWe see here that Chapter 150 is the most positive and Chapter 10 is the most negative. In Chapter 10, the psalmist laments about the wicked and asks God to “hear the desire of the afflicted.” Here is a sample:\n\n“His mouth is full of lies and threats; trouble and evil are under his tongue. He lies in wait near the villages; from ambush he murders the innocent. His eyes watch in secret for his victims; like a lion in cover he lies in wait.”\n\nChapter 150 is a short one of praise to God. Here’s a sample:\n\n“Praise him with trumpets. Praise him with harps and lyres. Praise him with drums and dancing. Praise him with harps and flutes. Praise him with cymbals. Praise him with loud cymbals. Praise the Lord, all living creatures!”\n\nNext let’s take another look at the sentiment using the sentimentr package. It has some nice features such as valence shifters, which are described on the package GitHub page as follows:\n\n“So what are these valence shifters? A negator flips the sign of a polarized word (e.g., ‘I do not like it.’). An amplifier (intensifier) increases the impact of a polarized word (e.g., ‘I really like it.’). A de-amplifier (downtoner) reduces the impact of a polarized word (e.g., ‘I hardly like it.’). An adversative conjunction overrules the previous clause containing a polarized word (e.g., ‘I like it but it’s not worth it.’).”\n\nThe sentimentr GitHub page also discusses the equations used to calculate sentiment. With a bit of work we could apply similar valence shifters with the tidytext package, but it’s nice that it’s automated in sentimentr.\nNote that by default this package uses the Jockers & Rinker sentiment dictionary, although it can be swapped out with an alternate.\n\npsalms_sentiment_w_valence <- chapters %>%\n    get_sentences() %$%\n    sentiment_by(text, by = ch_num)\n\n\n\n\n\n\n\n\n\n\n\n\nNow, after taking valence shifters into account, Chapter 67 is the most positive while Chapter 10 is still the most negative. Chapter 67 is a short chapter (only 7 verses), a song written for the director of music. Here is a sample of verses 3 and 4:\n\n“May the peoples praise you, God; may all the peoples praise you. May the nations be glad and sing for joy, for you rule the peoples with equity and guide the nations of the earth.”\n\nChapter 67 seems to have a similar sentiment as Chapter 150, which we identified in the previous analysis.\nAdditionally, between the two analyses, Chapter 13 switched from a very slight negative sentiment (-0.0038) to a somewhat positive sentiment (0.1513). This chapter had the largest score change between the two analyses.\nHave comments or feedback? Message me on Twitter: DrAmandaRP"
  }
]