[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2019-01-25-rstudio-conf-2019-recap/index.html",
    "href": "posts/2019-01-25-rstudio-conf-2019-recap/index.html",
    "title": "rstudio::conf recap",
    "section": "",
    "text": "The overview: I had the pleasure of attending rstudio::conf 2019 (in Austin, TX), a conference about all things R and RStudio. Over 1700 people attended the main conference, which consisted of three tracks (so my notes below cover only a third of the conference talks). There were an additional two days of tutorials and a two-day hackathon, which I did not attend. The 2020 conference will be in San Francisco Jan 27-30.\nMy thoughts: rstudio::conf (and also the yearly UseR! conference) is a great way to stay up to date on the latest and greatest R packages (there are currently 13K+ packages on CRAN) and to meet others excited about #rstats. They also gave the most useful swag that I’ve ever received at a conference: a binder full fo R package cheat sheets! … in addition to hex stickers and t-shirts. A big theme was using R in production; I found the talk about the use of R at T-mobile interesting. Additionally, the talks by Garrett Grolemund and Yihui Xie about rmarkdown/pagedown and the keynote by David Robinson, The Unreasonable Effectiveness of Public Work, motivated me to create this blog site!\nResources: Recorded videos of all talks are posted here. Additionally, links to slides can be found at here. And, check out the Twitter hashtag, #rstudioconf.\nRead more: Below are my notes from the talks that I attended. The post is a bit long, so I’ve divided it into the sections listed in the right margin."
  },
  {
    "objectID": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#viz",
    "href": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#viz",
    "title": "rstudio::conf recap",
    "section": "Visualization",
    "text": "Visualization\n3D mapping, plotting, and printing with rayshader, Tyler Morgan-Wall (Institute for Defense Analysis)\n\nRayshader: building realistic 3D maps from elevation data\nCan overlay images onto map. Animations possible. 3-D printing of maps.\nWatch the video for lots of cool examples.\nNext version will contain 3D ggplots\n\ngganimate Live Cookbook, Thomas Lin Pedersen (Software Engineer, RStudio)\n\ngganimate is an implementation of the grammar of animated graphics, extension to ggplot2.\nRewritten completely in the last year.\nNow available on CRAN!\nHow to use:\n\nTransitions: you want your data to change (transition_reveal: multiple enter/exit options )\nViews: you want your viewpoint to change\nShadows: you want the animation to have memory\n\nDocumentation: https://youtu.be/21ZWDrTukEs\nSee slides for examples."
  },
  {
    "objectID": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#modeling",
    "href": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#modeling",
    "title": "rstudio::conf recap",
    "section": "Modeling",
    "text": "Modeling\nIntroducing MLflow, Kevin Kuo (Software Engineer, RStudio)\n\nMLfow: an open source platform for the machine learning lifecycle.\nMotivation:\n\nKeeping track of what you did (trying different hyper-params, going back to a previous experiment, etc)\nReproducability\nLots of different modeling packages and ways to deploy them.\n\n\nSolving the model representation problem with broom, Alex Hayes (Tidyverse Intern, RStudio)\n\nModel representation problem: Need a standard way to represent models.\nbroom tools:\n\ntidy(): summarize info about fit components. Broom cleans up fit for us.\nglance(): report goodness of fit measures\naugment(): add info about observations to a dataset – not currently defined yet.\n\nPrint really pretty table of fit desc: tidy(fit) %&gt;% kable2()\n\nCompare different models using purrr. (See code on slides)\nReal power: working with multiple models at once.\nResource: broom.tidyverse.org\nSee slides!!!\n\nParsnip - a tidy model interface (building models), Max Kuhn (Engineer, RStudio)\n\nWhy parsnip name? White carrot (a play on caret)\nMotivation for parsnip:\n\nDifferent packages have different modeling interfaces.\nFunctions run na.omit silently. Delete data.\n\nparsnip has a tidy interface\nSimilar to broom, nicely prints output.\nEx: set_engine(“glmnet”) , however, engine doesn’t necessarily have to be an R engine.\nNext steps:\n\nAdd more models and classes of models\nFormalize the API and tools for others to add parsnip models to their packages\n\n\nWhy Tensorflow Eager Execution matters, Sigrid Keydana\n\nTypically a static graph is generated. New way: eager execution\nSee blog articles."
  },
  {
    "objectID": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#other",
    "href": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#other",
    "title": "rstudio::conf recap",
    "section": "Other R Packages",
    "text": "Other R Packages\nMelt the clock: tidy time series analysis, Earo Wang (PhD student, Monash University)\n\nPackages:\n\ntsibble: to tidy and transform ts data\nmable: table of models\nfable: tidy forecasting (table of forecasts)\n\nCan use familiar broom functions on a mable: tidy, glance, augment. Then use geom_forecast (ggplot) to visualize.\n\nWorking with categorical data in R without losing your mind, Amelia McNamara (Assistant professor, University of St Thomas)\n\n“Practical Data Science for Statistics”: collection of papers. Avail on Github.\nIn R, categorical data is represented as factors.\nFactors are nice for 1) modeling, 2) reordering items in ggplot.\nPaper: Wrangling categorical data in R. Shows how factors behave unexpectedly.\nUse read_csv instead of read.csv! Better options for data formats.\nforcats: package to easily work with factors. Solves many of the issues that base R has with factors.\n\nSee her cheat sheet on RStudio website: syntax comparison.\nsummary() is your friend (along with testthat) to make sure data isn’t changing unexpectedly.\n\nBuilding an A/B testing analytics system with R and Shiny, Emily Robinson (Data Scientist, DataCamp)\n\nfunneljoin - R package for common A/B testing analyses."
  },
  {
    "objectID": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#doc",
    "href": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#doc",
    "title": "rstudio::conf recap",
    "section": "Documentation",
    "text": "Documentation\nR Markdown: The Bigger Picture, Garrett Grolemund (Data scientist and Educator, RStudio)\n\nCo-other of R Markdown the Definitive Guide\nReplication crisis example: Estimate: 75%-90% of preclinical results cannot be reproduced, which costs $28 billion per year in the US.\nRmarkdown provides a map for other scientists to reproduce your results.\n\nPagedown: Creating Beautiful PDFs with R Markdown, Yihui Xie (Software Engineer, RStudio) – collaborator: Romain Lesur\n\nStatus: package is still experimental (obtain from github)\npagedeown creates paged documents (e.g. PDF) from web pages.\nChrome or chromium is recommended browser to view.\nCan also use to create:\n\nbusiness cards.\nResume (e.g. https://pagedown.rbind.io/html-resume)\nPosters\nLetters\nBooks\n\nSlides: http://bit.ly/pagedown\n\nIntroducing the the gt Package, Rich Iannone (Software Engineer, RStudio) @riannone\n\ngt lets you build pretty display tables with easy-to-use functions for HTML, LaTeX and RTF.\nCurrently on Github (not yet on CRAN)\ntibble %&gt;% gt() or dataframe %&gt;% gt()\nSee slides for an example of sending an email (with a table) from RStudio."
  },
  {
    "objectID": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#programming",
    "href": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#programming",
    "title": "rstudio::conf recap",
    "section": "General Programming",
    "text": "General Programming\nDemocratizing R with Plumber APIs, James Blair (Solutions Engineers, RStudio)\n\nplumber: Easily create API endpoints around R functions.\nInteracts with OPENAPI (Swagger)\nUse case: Use plumber to expose R models via an API\nMost recent updates avail in github repo\nResources:\n\nPlumber: www.rplumber.io\nopenAPI: https://swagger.io/docs/specification/about\nGithub: github.com/sol-eng/plumber-model\n\n\nVctrs: tools for making size and type consistent functions, Hadley Wickham (Chief Scientist, RStudio) @hadleywickham\n\nvctrs: a new package that provides tools to ensure that functions behave consistently with respect to inputs of varying length and type.\nBase R: When combining objects of different classes using c(), the following order of precidence occurs: Logical -&gt; integer -&gt; double -&gt; character (combine 2 types in an atomic vector and you get the type on the right)\nvctrs is more strict than an atomic vector in base R.\nWill be integrated into tidyverse packages (behind the scenes) during 2019.\n\nTidy eval context, Jenny Bryan (Software Engineer, RStudio)\n\nTidy evaluation is a toolkit for metaprogramming (code that writes/mutates code) in R.\nTidyverse itself does alot of metaprogramming (behind the scenes).\nSimilar to nonstandard evaluation or unquoted variable names.\nPackage: rlang (rlang.r-lib.org) is a developer facing package. Most people will not need to use.\nMight need enquo or !! if passing variables to tidyverse function.\nMy perspective: This is more in the low level weeds than I need.\n\nBox plots: a case study in debugging and perseverance, Kara Woo (Research Scientist, Sage Bionetworks)\n\nTalked through an example of a difficult debugging problem (for the ggplot2 package). * Advice:\n\nUse a reprex (minimal reproducible example) to determine/reproduce problem.\nUse debug function. (Let’s you step through function.)\n\nHow do you know when you are done? Don’t let perfect be the enemy of good."
  },
  {
    "objectID": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#interop",
    "href": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#interop",
    "title": "rstudio::conf recap",
    "section": "Interoperability with other languages",
    "text": "Interoperability with other languages\nScaling R with Spark, Javier Luraschi (Software Engineer, RStudio)\n\nsparklyr package avail on cran (also need spark on machine)\nConnecting to Spark: “Connections” pain in the upper right tab. -&gt; new connection -&gt; spark. Click the “Connect from” dropdown to see interaction options (e.g.: command line, R notebook)\nTo start: sc &lt;- spark_connect(master=”local”)\nCan use dplyr or sql code for data manipulation/query\nNew features:\n\nIntroduced pipelines. Can export to be used by ppl working in other languages.\nSpark structured streams: process streaming data with R dataframes. Apply usual dplyr transformations.\n\nCurrently working on:\n\nSupport for Apache arrow. Install R arrow package from github.\nXgboost models in spark\n\nResources:\n\nDocs: spark.rstudio.com\nBlog: blog.rstudio.com/tags/sparklyr\nStackoverflow: stackoverflow.com/tags/skparklyr\n\n\nUrsa Labs and Apache Arrow in 2019: Infrastructure for next-gen data science toolbox, Wes McKinney (Ursa Labs, Python Pandas Creator)\n\nhttps://ursalabs.org\nR and Python programmers solving many of the same problems.\nVision: multicore algorithms, lazy eval, sophisticated memory management, interoperable memory models, interchangeable between languages.\nArrow: founded in 2016. Language agnostic, open standard in-memory format for data frames. Tools (languages) share arrow memory. Goals: faster access to data, move data efficiently, compute analytics fast.\nArrow for R: Building bindings for R.\nPlans to improve the feather package. The feather file format is readable by both R and Python."
  },
  {
    "objectID": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#production",
    "href": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#production",
    "title": "rstudio::conf recap",
    "section": "R in Production",
    "text": "R in Production\nAPI Development with R and Tensorflow at T-Mobile, Heather Nolis & Jacqueline Nolis (Machine Learning Engineer & Data Scientist, nolisllc) {#tmobile}\n\nT-mobile has 70 million customers (+ additional from merger with Sprint)\nNLP models to classify incoming customer message (including account info) to give live agent a heads up about the likely topic. (They used a CNN via the keras R package.)\nTheir workflow steps: First Rmarkdown for exploratory data analysis. Second show model to business people using a shiny demo to get them interested. Success: Given millions of $ to put it into production.\nWas told: “If you want to do ML in production, you have to use Python.” Idea: Treat R like a real programming language (because it is one)! Didn’t want to re-write everything. Steps:\n\nTurn R into an API using plumber\nUse docker images (rocker)\nPlumber doesn’t support https. Used an appache2 server to reroute.\nContainer was too big. Swapped miniconda for anaconda. Removed RStudio & some unnecessary Linux, R, and python packages.\n\nThis model is now deployed and used in production at T-mobile.\nTheir docker container is now open source: https://github.com/tmobile/r-tensorflow-api\nSlides: nollisllc.com/rstudio19\n\nKeynote: Tareef Kawaf (President, RStudio),\n\nRStudio maintains 170 R packages\nMaking the life of an SA better: RStudio Server Pro, RStudio package manager.\nRstudio connect to publish Shiny applications, R Markdown reports, Plumber APIs, dashboards, plots, and more.\n\nKeynote: Shiny in Production, Joe Cheng (CTO of RStudio)\n\nCloud.r-project.org: cran mirror managed by RStudio. Download logs available (for that mirror).\nQ: Can shiny be used for production. Ans: Yes! (It’s quite easy.) Challenges: Cultural (shiny apps usually developed by R users, who aren’t necessarily SWEs). Organizational (IT and management can be skeptical). Technical (shiny lowers bar for creating web app… but not easy to automated testing, load testing, profiling, and deployment … but RStudio has worked on making this better).\nPet peeve: “R is not a real programming language.”\nNew tools for shiny in production:\n\nRStudio Connect - Shiny serving with push-button deployment\nShinytest (now on cran) - automated ui testing for shiny\nShinyloadtest - load testing for shiny\nprofvis - profiler for R\nPlot caching - in Shiny 1.2. Speed up plots. Use if plots are 1) slow, 2) significant fraction of total amt of time the app spending thinking, 3) most users likely to request the same few plots.\nAsync - last resort for slow portions.\n\nUse shinyloadtest to see if it’s fast enough (generates large amounts of traffic). If not, use profvis to see what’s making it slow. Then optimize: move work out of shiny (very often). Make code faster (very often), using caching (sometimes), use async (occasionally). Repeat.\nDeploy apps to either RStudioConnect or Shiny Server.\nReading from feather files is faster than csv.\nResources:\n\nBook: Shiny in Production (in progress)\nSlide deck\n\n\nR in Production Mark Sellers (Data Engineer, Mango Solutions)\n\nAuthored: Field Guide to the R Ecosystem\nBiggest challenge to using R in production: cultural, not technical."
  },
  {
    "objectID": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#rstudio",
    "href": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#rstudio",
    "title": "rstudio::conf recap",
    "section": "RStudio Updates",
    "text": "RStudio Updates\nNew Language Features in RStudio 1.2, Jonathan McPherson (Engineer, RStudio)\n\nGoals: more comprehensive R project workbench. Embrace other languages commonly used in R data science projects, reduce context switching.\nNon-goals: Become a general purpose IDE. Lose focus on R.\nDemo-ed the following languages within one R notebook.\n\nSQL: Connections tab -&gt; new connection -&gt; ODBC. Write SQL in RStudio (as a code chunk in R notebook, or in a SQL file).\nPython: reticulate package\nD3: r2d4 function\n\nBackground scripts: “source as local job” button to run job in the background. Can do other things in RStudio while it’s running. Multiple jobs can be run at once. Useful for long running computations.\nMake powerpoint presentation with RMarkdown: New file -&gt; presentation -&gt; PowerPoint\nStable release of 1.2 this spring."
  },
  {
    "objectID": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#education",
    "href": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#education",
    "title": "rstudio::conf recap",
    "section": "Education",
    "text": "Education\nR4DS online learning community, Jesse Mostipak (Data Scientist, Teaching Trust)\n\nStarted the R for Data Science (R4DS) slack group.\nRules: You will be kicked out of the group for being not-nice.\nwww.Rfordatasci.com\n@R4DScommunity\nTidyTuesday\n\nKeynote: Explicit Direct Instruction in Programming Education, Felienne (Associate Prof, Leiden University) @felienne\n\nTopic: How to teach programming?\nExamples of direct instruction:\n\nVocalize code snippets (when teaching kids)\nExplanation and practice works best (as opposed to explore). Skill -&gt; motivation.\n\nInteresting & entertaining talk for people interested in the teaching of coding.\n\nThe next million R users Carl Howe (Director of Education, RStudio)\n\nSurvey: See rstd.io/learning-r-survey. 3300 responses. (warning: sampling bias may be present)\nWho uses R? 110 countries responded.\nMost have advanced degrees.\n⅔ of R users use tidyverse today.\n15% of R users have no one else in their work group that knows R :(\nResources:\n\nRStudio teacher certification available.\nFree academic licensing for RStudio pro tools. Just send course syllabus from a certified academic institution. Research (instead of teaching) gets a 50% discount.\nData Science in a box: https://datasciencebox.org\nLots of free online books: R for Data Science, Advanced R, Blogdown, Hands on programming with R, Geocomputation in R.\nrstudio.cloud (free) primers for self learning.\n\n\nIntroductory Statistics with R: Easing the transition to software for beginning students, Kelly Nicole Bodwin (Faculty, Cal Poly) @kellybodwin\n\nThey use pre-lab exercises built in Shiny to let the students practice the statistics before having to code in R. Demo of a pre-lab exercise shown in talk.\nlearnr package was used to build shiny apps.\nGithub repo with exercises: github.com/kbodwin/Introductory-Statistics-Labs\nDemos:\n\nhttps://kbodwin.shinyapps.io/Lab_Exercise_CatVars2\nhttps://kbodwin.shinyapps.io/Lab_Exercise_tDist\nhttps://kbodwin.shinyapps.io/t_tests2\n\n\nTeaching data science with puzzles, Irene Steves (former intern, RStudio) @isteves\n\nPuzzle name: Tidies of March\nR package: tidiesofmarch\nSlides and code available at: bit.ly/ds-puzzles\n\nCatching the R wave how R and RStudio are revolutionizing statistics education in community colleges (and beyond), Mary Rudis (Math Dept Chair, Great Bay Community College)\n\nShiny apps for teaching stats:\n\nhttps://statistics.calpoly.edu/shiny\nhttps://shinyapps.science.psu.edu\n\nGithub: github.com/mrshrbrmstr/RStudioConf2019 (includes lesson plans)"
  },
  {
    "objectID": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#ds",
    "href": "posts/2019-01-25-rstudio-conf-2019-recap/index.html#ds",
    "title": "rstudio::conf recap",
    "section": "Data Science",
    "text": "Data Science\nUsing Data Effectively: Beyond Art and Science, Hilary Parker (Data Scientist, Stitch Fix)\n\nStitch Fix has very little data on both the person and the clothing item… traditional matrix factorization (collaborative filtering) does not work well. Soln: They added the “style shuffle” where customer flips through a bunch of clothes and says yes/no to whether they would wear it. This enabled traditional matrix factorization.\nR magik package for image processing.\n\nData Science as a Team Sport, Angela Bassa (Director of Data Science, iRobot) @angebassa\n\nJust because a data scientist can do everything, doesn’t mean they should.\nData engineer != data scientist\nWhen to grow? When you’d like to grow scope & maturity. Adding people will slow things down (additional complexity) unless systems are improved.\nWhen adding people: add specialization (people have different roles in the DS pipeline), add process (documentation, authentication, governance, data provenance, automation: testing, deployment), add resilience (hiring, methodical on-boarding, culture, diversity & inclusion)\nIdea: Documentation party … offsite … have pizza.\nHire both experts and interns (the latter question what experts have forgotten to question)\nWhere should DS team live? Embedded or centralized? Her answer: It doesn’t matter. The importance is how they interact and communicate. But, remember teams don’t scale.\nIn her experience the best model she’s found is 5-10 data scientists (of differing background/expertise) + ~3 data engineers on a team. If more then split into multiple teams.\n\nKeynote: The Unreasonable Effectiveness of Public Work, David Robinson (Chief Data Scientist, Data Camp) {#dr}\n\nWork shared publically is way more useful than work local on your computer.\nEffective ways to share: blog, tweet, contribute to open source, give talks, record screencasts, write a book.\nWhen you’ve given the same in-person advice 3 times, write a blog post.\nIf you start a data-related blog, tweet link to @drob and he’ll tweet about your first post.\nWhat to blog about?\n\nAny paper you’re written. (more exposure)\nCurrent events\nTidyTuesday\nTeach a concept\nBlog about something that you’re learning.\nExamples: #datablog hashtag on Twitter\n\nTwitter:\n\nOne for each blog post.\nPromote others’ work\nTidytuesday evaluation.\nWhat you’ve learned at a conference\nNot a great way to document knowledge for long term. Blog better for that.\nFrom:username to search within one person’s tweets.\n\nContribute to open source:\n\nSee Ten Steps to Becoming a Tidyverse Contributor by Nic Crane\nContribute to a beginner-friendly issue\nWrite R packages (See R Packages book)\n\nGive talks:\n\nSee Giving your first data science talk by Emily Robinson\nIntrovert tip: It makes networking at conferences easier.\nBe sure to publish your slides.\n\nRecord screencasts:\n\nLimitation: you need to be capable and confident enough to improvise.\nRachel Tatman does live coding on Twitch.\n\nWrite a book:\n\nYou need a good amount to say and some practice saying it.\nR bookdown package.\n\nSlides: bit.ly/drob-rstudio-2019\n\nPanel Discussion: Growth and Data Science: Individuals, leaders, organizations and responsibilities, Speakers: Hilary Parker (Data Scientist, Stitch Fix), Karthik Ram (Data Science Fellow, UC Berkeley), Angela Bassa (Director of Data Science, iRobot), Tracy Teal (Exec Dir, Carpentries), Eduardo (Data Science Leader, Instagram)\n\nSee Hilary’s Not so standard data science podcast\nWhat is the most important thing for success as a data scientist:\n\nFlexibility in tooling (no space for a tooling purist)… but know at least one language fluently. Softer skills (e.g. empathy, understanding user/reader)\nIf an organization is giving you a hard time about your lack of knowledge as a junior data scientist, consider a different company. Don’t shy away from saying “I don’t know.” Her interview guide has impossible questions to answer… she’s looking for humility to say “I don’t know.”\n\nHow to choose to become a DS lead vs staying technical:\n\nData scientists aren’t necessarily given opportunity to learn/grow leadership skills. Need to be more strategic to grow better leaders. Some DS’s stumble into a management/leadership position.\nAngela didn’t want to become a manager and wanted to stay in trenches at first, but now loves it (took the position b/c didn’t want to let her management down).. “Management is a skill that is learnable.” If you feel accomplishment from management, great rock it. If you don’t, then no worries don’t be a manger.\nIt takes personal growth to stop self-depricating yourself as a manager: “I just read email and schedule meetings.”\nFear: Skills will atrophy. Remember “loops are still loops.” Opportunity to develop additional skills. Redefine what success means: How can I enable people to do their best work?\n\nIn a growing organization that you’ve been at, what is the most important lesson to do/not do?\n\nDon’t hire DS if you don’t have data.\nAdapt as the business grows.\nInvest in systems that let people work effectively together\nDon’t over hire. No clear success criteria.\nFor machine learning projects, useful to have a PM that understand machine learning. There types of people can be rare.\n\nResponsibilities as Data scientists:\n\nData are artifacts…not ground truth.\nOwn your mistakes.\nMake work reproducible.\nSee datasciencemanifesto.org\n\nWhat is the most effective way that program management and DS works together:\n\nProduct has a road map. Make sure that DS also has a road map and that they are correlated.\n\nWhat are common ways that data scientists fail?\n\nBy not saying “no” enough then only providing a cursery analysis that has little impact.\nAssuming the PM knows enough to ask the question in the best way.\nCaring more about the stats method/model than solving the problem for the business."
  },
  {
    "objectID": "posts/2019-08-02-twitter-likes/index.html",
    "href": "posts/2019-08-02-twitter-likes/index.html",
    "title": "Shiny App: Twitter Likes",
    "section": "",
    "text": "I love that Twitter recently came out with bookmarks! My previous MO on Twitter was to ❤️ posts that contain information that I want to keep for later, most of which are #rstats tweets. I now have 700+ likes, which are time consuming to browse through when I need to find something. I didn’t find an easy way to search likes within Twitter itself, so I built a shiny app to do that using the rtweet and shiny R packages.\n\nrtweet Package\n\n\n\nTo grab the tweets, there is a little setup with Twitter that is necessary. The setup is described on the rtweet website.\nDuring setup, I ran into a bit of an issue authenticating my Twitter app. I’m working in RStudio on an AWS instance (via a Chromebook), which has a known issue. The issue and its work-around are described by the package author here. After that speed bump was crossed, the rtweet package was extremely easy to use!\n\n\nMy Twitter Likes\nTo get a Twitter user’s favorites (aka likes), use the get_favorites function. Below I’m going to grab my most recent five favorites:\n\nfavorites &lt;- get_favorites(\"DrAmandaRP\", n = 5, token = token)\n\nThe resulting tibble has 91 columns!\nFor fun, here is a wordcloud composed of all of my Twitter likes (created using the tidytext and wordcloud2 packages):\n\nlibrary(wordcloud2)\nlibrary(tidytext)\nfavorites &lt;- get_favorites(\"DrAmandaRP\", n = 1000, token = token)\nfavorites %&gt;% \n  select(text) %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  anti_join(stop_words) %&gt;%\n  filter(!word %in% c(\"t.co\", \"https\", \"http\")) %&gt;%\n  wordcloud2()\n\n\n\n\n\n\n\n\nshiny app for searching\nI zeroed in on the following fields: status_id, created_at, text, hashtags, name, screen_name. I put them in a table in a shiny app for easy browsing. Check it out here! It’s nothing fancy, but I think it’s going to come in handy.\nIf you’d like to make your own app, my code is available on GitHub.\nIt would also be nice to add functionality to search Twitter bookmarks, but apparently at the time of this post, reading bookmarks isn’t yet available in the Twitter API. I’m keeping an eye on this rtweet issue for updates."
  },
  {
    "objectID": "posts/2019-03-10-rstats-bingo/index.html",
    "href": "posts/2019-03-10-rstats-bingo/index.html",
    "title": "Statistics Bingo",
    "section": "",
    "text": "I just wrapped up another round of teaching stats. I co-teach two courses per year as an adjunct instructor: one class that covers intro stats and the other that hits higher level concepts. Both courses move at a high pace; they’re intended to be a survey course for professionals with a STEM background. Some students have a math background, but have never taken a statistics course. Other students have had statistics a long time ago in their undergraduate studies, but want to brush up on the material.\nIn both classes, by the time we get to the end, the students appreciate a light hearted activity. I found bingo to be the perfect activity to reveiw material and have a little fun. Jenny Bryan and Dean Attali have an R Shiny app for creating such bingo games. Their site has a handful of pre-populated game themes such as “boring meeting” and “bad data”. Or you can choose to create your own by pasting a list of words.\nI created my own bingo questions that I ask during the game and answers that are written on the bingo cards. (Tip: Don’t forget to bring the associated list of questions to class like I did!) I’ve pasted the questions and answers below that I used for each class.\nLastly, don’t forget the sugar! M&M’s make great bingo chips. And, cookies decorated with statistics are a crowd-pleaser."
  },
  {
    "objectID": "posts/2019-03-10-rstats-bingo/index.html#bingo-questions-for-probability-statistics-i",
    "href": "posts/2019-03-10-rstats-bingo/index.html#bingo-questions-for-probability-statistics-i",
    "title": "Statistics Bingo",
    "section": "Bingo Questions for Probability & Statistics I",
    "text": "Bingo Questions for Probability & Statistics I\n\n\n\n\n\n\n\nQuestions\nAnswers\n\n\n\n\n1. The R function used to find the area under the normal curve\npnorm\n\n\n2. For a discrete random variable: \\(E(X) = \\sum_i^n X_i\\) * ____\n\\(P(X)\\)\n\n\n3. Square root of the variance\nstandard deviation\n\n\n4. Mathematical function used to count number of possibilities: without replacement, order doesn’t matter.\nchoose\n\n\n5. To obtain a ___ sample: Divide the population into at least two different sub-populations based on some characteristic of interest. A sample is drawn from each sub-population.\nstratified\n\n\n6. The Central Limit Theorem says that the distribution of the mean converges to this distribution as \\(n\\) goes to infinity (variance must be defined).\nnormal\n\n\n7. The ____ distribution approximates the Binomial when \\(n\\) is large and \\(p\\) is small. In this case we set the parameter \\(\\lambda=np\\).\nPoisson\n\n\n8. \\(P(A \\cup B) = P(A) + P(B) -\\) ___ ?\n\\(P(A \\cap B)\\)\n\n\n9. Discrete distribution that models the number of successes in \\(n\\) Bernoulli trials, where \\(p\\) is the probability of success.\nBinomial\n\n\n10. The test statistic in the Test of Independence and The Goodness of Fit has a ___ distribution.\nChi-Square\n\n\n11. This distribution has PMF \\(P(X=k) = p^k(1-p)^{1-k}\\)\nBernoulli\n\n\n12. Used to control the type I error of a hypothesis test\nsignificance level (\\(\\alpha\\))\n\n\n13. In hypothesis testing, we reject the null hypothesis if the test statistic falls in the ____ region.\ncritical\n\n\n14. Bayes Rule: \\(P(A | B) =\\) ____ * \\(\\frac{P(A)}{P(B)}\\)\n\\(P(B | A)\\)\n\n\n15. The area under the entire PDF curve\n1\n\n\n16. The continuous distribution that looks like a bell curve (except that it has fat tails), converges to normal as \\(n\\) increases, and is used when modeling data with small sample size.\nStudent’s \\(t\\)\n\n\n17. The maximum minus the minimum\nrange\n\n\n18. The most frequent observation in a data sample\nmode\n\n\n19. The function \\(F(X) = P(X \\le x)\\).\nCDF\n\n\n20. A ___ provides a range for which we are \\((1-\\alpha)\\)% confident contains the true value of the population parameter. In class we considered such ranges for the true mean, \\(\\mu\\), and for proportions.\nconfidence interval\n\n\n21. The ___ error occurs when the null hypothesis is true, but we reject it in favor of the alternative hypothesis.\nType I\n\n\n22. The ___ of a hypothesis test is the probability that we reject the null hypothesis when it is in fact false.\npower\n\n\n23. \\(1 - P(A) =\\) the probability of the ___ of \\(A\\).\ncompliment"
  },
  {
    "objectID": "posts/2019-03-10-rstats-bingo/index.html#bingo-questions-for-probability-statistics-ii",
    "href": "posts/2019-03-10-rstats-bingo/index.html#bingo-questions-for-probability-statistics-ii",
    "title": "Statistics Bingo",
    "section": "Bingo Questions for Probability & Statistics II",
    "text": "Bingo Questions for Probability & Statistics II\n\n\n\n\n\n\n\nQuestions\nAnswers\n\n\n\n\n1. Let {\\(X_i\\)} for \\(i=1..n\\) partition the sample space. Bayes rule says that \\(f(X_i | Y) = \\frac{f(Y | X_i)f(X_i)}{\\sum_{i=1}^{n}f(Y | X_i)f(X_i)}\\). The ___ law is used in denominator when the marginal, \\(f(Y)\\), is unknown.\nLaw of Total Probability\n\n\n2. Maximization of the log likelihood function (take derivative and set equal to 0)\nMLE\n\n\n3. Joint probability density function for two or more variables\nJoint PDF\n\n\n4. Result of integrating \\(f(X,Y)\\) with respect to \\(Y\\), over the support of \\(Y\\)\nmarginal of \\(X\\)\n\n\n5. An alternative to frequentist statistical analysis\nBayesian statistics\n\n\n6. \\(P(A \\cap B) = P(A | B)\\) * ____\n\\(P(B)\\)\n\n\n7. Discrete distribution that models number of trials until 1 success\nGeometric (special case of negative binomial)\n\n\n8. Continuous distribution use to model time until events occur\nGamma\n\n\n9. True or false: \\(Cov(X,Y)=0\\) implies \\(X\\) and \\(Y\\) are independent.\nFALSE\n(TRUE if \\(X\\) and \\(Y\\) are jointly normally distributed. Independence implies \\(cov=0\\) ).\n\n\n10. \\(E(XY) = E(X)E(Y)\\) when X and Y are independent\nTRUE\n\n\n11. \\(Var(X) = E(X^2) -\\) ____\n\\(E(X)^2\\)\n\n\n12. \\(V(aX+b) =\\) ___\n\\(a^2V(X)\\)\n\n\n13. Name of this function: \\(E(X^{tx})\\)\n(Note: obtain \\(k^{th}\\) moment by taking \\(k^{th}\\) derivative and evaluating at \\(t\\)=0)\nMoment Generating Function\n\n\n14. Used in sampling from strata to ensure that the proportion of observations in the sample mimic the population proportions.\nProportional allocation\n\n\n15. Used to control the type I error of a hypothesis test\nSignificance level (\\(\\alpha\\))\n\n\n16. In Bayesian statistics, the population parameters are considered to be ____.\nrandom variables\n\n\n17. \\(P(A | B) =\\) ____ * \\(\\frac{P(A)}{P(B)}\\)\n\\(P(B | A)\\)\n\n\n18. The result of taking the derivative of the CDF with respect to the random variable (for a single random variable)\nPDF\n\n\n19. The second non-central moment\nVariance\n\n\n20. If \\(E(\\hat{\\theta}) = \\theta\\) we say \\(\\hat{\\theta}\\) is ___.\nUnbiased"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I’m a data scientist living and working in Maryland, specifically the metro DC area. When I have time, I like to write about statistics and data science topics. If I haven’t posted in a while, it’s most likely because I’ve been distracted by a good book or decided to spend time outside. Feel free to reach out to me on Twitter or LinkedIn!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Amanda's Data Science Blog",
    "section": "",
    "text": "Book Recommendations via Neural Collaborative Filtering\n\n\nA recommender systems model built with R Keras\n\n\n\nApr 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstudio::global(2021) |&gt; summarize()\n\n\nMy notes from the 2021 RStudio (virtual) Conference\n\n\n\nJan 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMirror, mirror on the wall, does my test data reflect my training data at all?\n\n\nA closer look at the train-test split\n\n\n\nNov 8, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShiny R-Ladies\n\n\nTutorial on R Shiny at a Baltimore R-Ladies meetup\n\n\n\nFeb 7, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDC R Conference Recap\n\n\nA summary of the 2019 DC R Conference at Georgetown University.\n\n\n\nNov 12, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nShiny App: Twitter Likes\n\n\nAnalysis of my Twitter likes via a wordcloud and a shiny app.\n\n\n\nAug 2, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nSentiment Analysis of Psalms\n\n\nSentiment analysis of Psalms using the tidytext and sentimentr packages\n\n\n\nJun 22, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing the hddtest R package\n\n\nAn R package for two sample hypothesis testing of high dimensional discrete data\n\n\n\nMay 24, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics Bingo\n\n\nBingo for a statistics course\n\n\n\nMar 10, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstudio::conf recap\n\n\nMy notes from 2019 rstudio::conf\n\n\n\nJan 26, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2019-05-24-new-r-package-hddtest/index.html",
    "href": "posts/2019-05-24-new-r-package-hddtest/index.html",
    "title": "Introducing the hddtest R package",
    "section": "",
    "text": "I’d like to introduce a package that I wrote recently, hddtest, for two sample hypothesis testing of high dimensional discrete data. The package is currently available on GitHub. It contains methods for both multivariate binary data and multinomial data.\nI’ll illustrate the use of one of its functions, the multinomial neighborhood test, by using a dataset included in hddtest called twoNewsGroups. It contains document term frequency matrices for two of the 20 newgroups. Each matrix has dimension 594 by 16214. The \\((i,j)\\) entry of each matrix is the count (term frequency) of the \\(j^{th}\\) word in the \\(i^{th}\\) document. The first matrix in the list contains 594 sampled documents from the rec.sport.baseball newsgroup. The second contains 594 sampled documents from the sci.med newsgroup.\nWe might like to compare two sets of documents to determine whether or not they come from the same newsgroup. To do this, we’ll perform a neighborhood test.\nWhat is a neighborhood test useful for? In testing the equality of parameters from two populations, it frequently happens that the null hypothesis is rejected even though the estimates of effect sizes are close to each other; however, these differences are so small that parameters may not be considered to be different in practice. Another issue is that although the use of p-values is a common measure to draw a conclusion about the population, one may be interested in the measure of indifference or inhomogeneity.\nWe’ll start by sampling two sets of 200 documents from the sci.med newsgroup. We’ll use this to simulate the null hypothesis being TRUE.\n\nlibrary(hddtest)\ndata(twoNewsGroups)\n\nnum_docs &lt;- 200\nrow_ids &lt;- 1:nrow(twoNewsGroups$sci.med)\ngroup_1 &lt;- sample(row_ids, num_docs)\ngroup_2 &lt;- sample(row_ids[-group_1], num_docs)\n\nNext for each of the two groups, sum the 200 term frequency vectors together. They will be the two multinomial vectors that we test. We’ll store the result in a list called, vecs2Test.\n\nvecs2Test &lt;- list(NA, 2)\nvecs2Test[[1]] &lt;- twoNewsGroups$sci.med[group_1, ] %&gt;% \n  colSums() %&gt;% \n  matrix(nrow = 1)\nvecs2Test[[2]] &lt;- twoNewsGroups$sci.med[group_2, ] %&gt;%\n  colSums() %&gt;% \n  matrix(nrow = 1)\n\nNow test the null hypothesis that the associated multinomial probability vectors are within some neighborhood, delta, of each other (against the alternative that they are not). We can then use this result to infer whether the sets of documents are likely from the same newsgroup.\n\nvecs2Test %&gt;% multinom.neighborhood.test(delta = 60)\n#&gt; $statistic\n#&gt; [1] 7.811123\n#&gt; \n#&gt; $pvalue_delta\n#&gt;      [,1]\n#&gt; [1,]    1\n\nHere we fail to reject the null hypothesis using a delta of 60. How to choose the appropriate delta? The answer may come from subject matter expertise about the problem domain. Or you can run a simulation to gain insight. Below we define a simulation function.\n\nsimulation &lt;- function(data, null_hyp, delta, reps = 30, num_docs = c(200, 200)){\n  \n   vecs2Test &lt;- list(matrix(NA, reps, ncol(data[[1]])), matrix(NA, reps, ncol(data[[1]])))\n   \n   for(i in 1:reps){\n     if(null_hyp){\n       # Sample two sets of num_docs from the SAME set of documents     \n       row_ids &lt;- 1:nrow(data[[2]])\n       group_1 &lt;- sample(row_ids, num_docs[1])\n       group_2 &lt;- sample(row_ids[-group_1], num_docs[2])\n       vecs2Test[[1]][i, ] &lt;- data[[2]][group_1, ] %&gt;% colSums()\n       vecs2Test[[2]][i, ] &lt;- data[[2]][group_2, ] %&gt;% colSums()\n       \n     }else{\n       # Sample num_docs from each of the two DIFFERENT sets of documents\n       vecs2Test[[1]][i, ] &lt;- data[[1]][sample(1:nrow(data[[1]]), num_docs[1]), ] %&gt;%\n                                      colSums()\n       vecs2Test[[2]][i,] &lt;- data[[2]][sample(1:nrow(data[[2]]), num_docs[2]), ] %&gt;%\n                                      colSums()\n     }\n   }\n   \n   #Perform the test:\n   result &lt;- vecs2Test %&gt;% multinom.neighborhood.test(delta = delta)\n   \n} #end simulation function\n\nNow run the simulation for varying values of delta (in the range 1 to 160) testing both the null and alternative hypotheses for 30 replications each. For the null hypothesis simulation, we sample documents from the same newsgroup. For the simulation of the alternative hypotheis, we sample documents from the two different newsgroups. The resulting plot shows one curve for each the 60 simulations (which compute the p-value at each value of delta.)\n\ndelta &lt;- 1:160\np.delta.null &lt;- simulation(data = twoNewsGroups, null_hyp = TRUE, delta = delta)$pvalue_delta\np.delta.alt  &lt;- simulation(data = twoNewsGroups, null_hyp = FALSE, delta = delta)$pvalue_delta\n\n\n\n\n\n\nNotice in the plot above that for delta in the range of about 40 to 100, the p-value is large when the the two sets of documents come from the same newsgroup (shown in blue) and small when the two sets of documents come from different newsgroups (shown in red). So, our previous choice of delta=60 seems reasonable for making the correct conclusion.\nThe methods for this package were developed in collaboration with my UMBC PhD thesis advisor, Dr Junyong Park, and published in 1 and 2. Full details about the statistics used and their distributions are documented in these papers. To see the full list of functions available in hddtest, see the GitHub README. Thanks for reading!"
  },
  {
    "objectID": "posts/2019-05-18-text-analysis-of-psalms/index.html",
    "href": "posts/2019-05-18-text-analysis-of-psalms/index.html",
    "title": "Sentiment Analysis of Psalms",
    "section": "",
    "text": "Psalms is a book in both the Christian Bible and the Jewish Tanakh. I thought it would be interesting to look at a sentiment analysis of the book. I chose to work with the “Good News Translation” of the bible as it does not have copyright issues like some of the other translations (per the American Bible Society). We’ll use the tidytext and sentimentr R packages to do the analysis. Let’s start by reading data from biblegateway.com (using rvest) and doing a bit of cleaning:\n\nget_chapter &lt;- function(chapter){ #Pull text and do a bit of cleaning:\n  \n  url &lt;- str_c(\"https://www.biblegateway.com/passage/?search=Psalms+\", \n               chapter, \n               \"&version=GNT\")\n  \n  html &lt;- read_html(url)\n  \n  text &lt;- html %&gt;% \n    html_node(\".passage-wrap\") %&gt;% \n    html_text() %&gt;%\n    str_extract(\"\\\\(GNT.{1,}\") %&gt;%\n    str_replace(\"\\\\(GNT\\\\)\",\"\") %&gt;%\n    str_extract(\"\\\\d.{1,}\") %&gt;%\n    str_replace_all(\"\\\\d{1,}\", \"\") %&gt;%\n    str_replace_all(\"\\\\[\\\\w{1}\\\\]\",\"\")\n}\n\n#Use the function defined above to read all 150 chapters.\nchapters &lt;- map_chr(1:150, get_chapter) %&gt;% \n  tibble::enframe(name = \"ch_num\", value = \"text\") \n\n\nPositive and Negative Word Lexicon\nNext, we’ll use the tidytext package to tokenize (i.e. split the text by words) and join our data with a dictionary of sentiment words. For more information about text analysis using tidytext, see Text Mining with R: A Tidy Approach by Julia Silge and David Robinson. tidytext comes with sentiment dictionaries, but I’m going to use the Jockers & Rinker sentiment dictionary from the lexicon package to better compare with a follow-on analysis using the sentimentr package. This dictionary contains positive and negative words and an associated sentiment score in the range [-1, 1]. A value of 1 is the most positive, 0 is neutral, and negative 1 is most negative.\nLet’s first look at a comparison word cloud to compare the frequency of the positive and negative words in the book.\n\n#Tokenize and join with sentiment lexicon:\npsalms_sentiment_jockers_rinker &lt;- chapters %&gt;% \n  unnest_tokens(word, text) %&gt;%  #tokenize by words\n  #anti_join(stop_words) %&gt;%\n  left_join(lexicon::hash_sentiment_jockers_rinker, \n             by = c(\"word\" = \"x\"), \n             drop = FALSE) %&gt;%\n  mutate(y = replace_na(y, 0)) \n\n#Draw the comparison cloud:\npsalms_sentiment_jockers_rinker %&gt;%\n  filter(abs(y) &gt; 0) %&gt;%\n  mutate(pos = y&gt;0, neg = y&lt;0) %&gt;%\n  select(-ch_num, -y) %&gt;%\n  group_by(word, pos, neg) %&gt;%\n  summarize(cnt = n()) %&gt;%\n  mutate(Positive = cnt * pos, Negative = cnt * neg) %&gt;%\n  ungroup %&gt;%\n  select(word, Positive, Negative) %&gt;%\n  as.data.frame() %&gt;%\n  column_to_rownames(\"word\") %&gt;%\n  comparison.cloud(title.colors = \"black\")\n\n\n\n\n\n\n\n\nSentiment Analysis by Chapter\nThere is some flexibility in the method that we may choose to compute the sentiment. We could sum the sentiment scores for the words in each chapter, which introduces a relationship between sentiment score and chapter length. Or we could compute the average sentiment over the words in the chapter, either choosing to ignore or include neutral words (i.e. words with score of 0). The inclusion of neutral words in the calculation of the average would dampen the overall sentiment score of the chapter. I think the choice depends on what makes the most sense for each application.\nIn the interest of better comparing this first calculation to a second calculation using the sentimentr package, I’m going use the mean chapter sentiment (including neutral words). That is, the sentiment for the \\(j^{th}\\) chapter is\n\\[S_j =  \\frac{1}{n_j} \\sum_{i=1}^{n_j} s_{ij}\\]\nwhere \\(n_j\\) is the word count for chapter \\(j\\) and \\(s_{ij}\\) is the sentiment score for the \\(i^{th}\\) word in the \\(j^{th}\\) chapter.\n\n#Compute sentiment:\npsalms_sentiment_jockers_rinker %&lt;&gt;%\n  group_by(ch_num) %&gt;%\n  summarize(avg_sentiment = mean(y))\n\n\n\n\n\n\nWe see here that Chapter 150 is the most positive and Chapter 10 is the most negative. In Chapter 10, the psalmist laments about the wicked and asks God to “hear the desire of the afflicted.” Here is a sample:\n\n“His mouth is full of lies and threats; trouble and evil are under his tongue. He lies in wait near the villages; from ambush he murders the innocent. His eyes watch in secret for his victims; like a lion in cover he lies in wait.”\n\nChapter 150 is a short one of praise to God. Here’s a sample:\n\n“Praise him with trumpets. Praise him with harps and lyres. Praise him with drums and dancing. Praise him with harps and flutes. Praise him with cymbals. Praise him with loud cymbals. Praise the Lord, all living creatures!”\n\nNext let’s take another look at the sentiment using the sentimentr package. It has some nice features such as valence shifters, which are described on the package GitHub page as follows:\n\n“So what are these valence shifters? A negator flips the sign of a polarized word (e.g., ‘I do not like it.’). An amplifier (intensifier) increases the impact of a polarized word (e.g., ‘I really like it.’). A de-amplifier (downtoner) reduces the impact of a polarized word (e.g., ‘I hardly like it.’). An adversative conjunction overrules the previous clause containing a polarized word (e.g., ‘I like it but it’s not worth it.’).”\n\nThe sentimentr GitHub page also discusses the equations used to calculate sentiment. With a bit of work we could apply similar valence shifters with the tidytext package, but it’s nice that it’s automated in sentimentr.\nNote that by default this package uses the Jockers & Rinker sentiment dictionary, although it can be swapped out with an alternate.\n\npsalms_sentiment_w_valence &lt;- chapters %&gt;%\n    get_sentences() %$%\n    sentiment_by(text, by = ch_num)\n\n\n\n\n\n\nNow, after taking valence shifters into account, Chapter 67 is the most positive while Chapter 10 is still the most negative. Chapter 67 is a short chapter (only 7 verses), a song written for the director of music. Here is a sample of verses 3 and 4:\n\n“May the peoples praise you, God; may all the peoples praise you. May the nations be glad and sing for joy, for you rule the peoples with equity and guide the nations of the earth.”\n\nChapter 67 seems to have a similar sentiment as Chapter 150, which we identified in the previous analysis.\nAdditionally, between the two analyses, Chapter 13 switched from a very slight negative sentiment (-0.0038) to a somewhat positive sentiment (0.1513). This chapter had the largest score change between the two analyses.\nHave comments or feedback? Message me on Twitter: DrAmandaRP"
  },
  {
    "objectID": "posts/2019-11-12-dcr/index.html",
    "href": "posts/2019-11-12-dcr/index.html",
    "title": "DC R Conference Recap",
    "section": "",
    "text": "The overview: DC R 2019, hosted by Lander Analytics, was held Nov 7-9 at Georgetown University. The short action-packed conference consisted of one-day workshops and two days of talks. Find the videos on the DC R website (available soon) and associated tweets via the #rstatsdc hashtag on Twitter. Also, see Jared Lander’s blog post for a higher level overview of the conference with lots of fun pics.\nHighlights for me personally included Malorie Hughes’ presentation of flexdashboard. Somehow in the hype of R shiny, I had missed this dashboard tool. I was also excited to learn about Dr Dasgupta’s coursedown package, which I’m looking forward to testing out for the class that I teach. Another highlight was learning more about the implementation of Google’s BERT model in R by Jon Harmon & Jonathan Bratt.\nRead more: Below are my notes from selected talks. I’ve divided it into the following sections: Communication, Modeling, Spatial Analysis, General R Programming, and A Few Remaining Random Bits."
  },
  {
    "objectID": "posts/2019-11-12-dcr/index.html#comms",
    "href": "posts/2019-11-12-dcr/index.html#comms",
    "title": "DC R Conference Recap",
    "section": "Communication (Dashboards, Websites, and Data Viz)",
    "text": "Communication (Dashboards, Websites, and Data Viz)\nCoursedown: Managing Course Materials Using R Techniques, Dr Abhijit Dasgupta, @webbedfeet\n\nPackage available on GitHub.\nUse drake to tie it all together\n\nDashboarding Like a Boss, Malorie Hughes, @data_all_day\n\nDon’t email your results. Create a dashboard!\nTalk featured a demo of flexdashboard. See more info about flexdashboard on RStudio’s website.\n\nCreate using R Markdown. Just requires a different output type in the YAML header.\nSee her GitHub repo for templates\nTips for making your output look pretty on the dashboard:\n\nUse printr to make printing R output look better. Less effort than knitr::kable().\nUse summarytools. It’s not great YET for R Markdown, but keep tabs on it.\nFor model output, use stargazer\nDT::datatable() for interactive table (see advanced options in this function for fancier options for the table)\nFor interactive plots: highcharter. Assign colors with colorize().\n\nShe included an export/download data button on her dashboard. See her code to learn how to do that.\nSee Malorie’s official DC R dashboard:\n\n\n\n\nUsing networkD3 and R to Visualize and Explore Relationships in Data, Dr Ami Gates, @DrGates309\n\nTalk focused on networkD3, which is nice for interactive networks.\nOther options for visualizing networks: igraph, ggnet2() (part of GGally), and visNetwork\narules package for association mining in R. She used it to find associated words considering both how often words appear together and also how often they appear in same dataset (in her example, how often in the same tweet)\narulesViz - for visualization of association rules\nShe used the igraph package to massage the data before giving it to networkD3\n\nBetter DataViz in ggplot2: Tips, Tricks, and Examples, Alex Engler, @AlexCEngler\n\nggplot2 + sf for bivariate plots: ggplot2::geom_sf(). See the Drawing beautiful maps with R, sf, and ggplot2 sf tutorial on r-spatial.org.\nggtext to add text to plots using markdown. Includes text formatting. Showed an example of how to highlight parts of title to match highlighted part of graph.\ngghighlight to highlight parts of graph with color (and grey out rest)\nggplot2::annotation_custom() to add a plot on top of another. Create a “grobe” and drop it somewhere. Use to zoom in to a map with a bounding box. (My comment: seems similar to cowplot).\ngeofacet::facet_geo() to create a plot per state, positioned in the location of the states on the map.\nggplot2 extensions: See www.ggplot2-exts.org/gallery and the rayshader package."
  },
  {
    "objectID": "posts/2019-11-12-dcr/index.html#modeling",
    "href": "posts/2019-11-12-dcr/index.html#modeling",
    "title": "DC R Conference Recap",
    "section": "Modeling",
    "text": "Modeling\nRBERT: Cutting Edge NLP in R, Jon Harmon, @JonTheGeek\n\nBERT: Bidirectional Encoder Representations from Transformers. Transfer learning for natural language processing. Created by Google. RBERT is the R implementation.\nOlder embeddings have one embedding per word. Can’t help this situation: “I saw the branch on the bank.” vs “I saw the branch of the bank.” BERT addresses this issue.\nRBERTviz: Visualize attention and PCA. Let’s you see how BERT is thinking. Showed an interactive demo— go watch the video!\nBERT has 12 layers.\nRBERT usable now (available on GitHub), but the Jon and Jonathan are making it more user-friendly. Goal: On CRAN by end of 2019\nWorks with tensorflow 2.0 (in testing)\nExample shown: Used RBERT to pull out features for xgboost model.\nSlides available here\n\nOptimizing Topic Models for Classification Tasks, Tommy Jones, @thos_jones\n\nPerforming PhD research to optimize LDA topic models\nCousin to LDA: Text embeddings (considered state of the art). In his words, LDA stopped being cared about after deep learning came along. However, he thinks the LDA work was never completed to determine a good LDA model. He thinks LDA might be competitive with deep neural nets if tuned appropriately.\nWalked through an example of classifying 20newsgroups\nTech stack: textmineR (maintained by Tommy), randomForest (simple so he could focus more on LDA), SigOptR, cloudml, magrittr, stringr, parallel (part of R-core)\nSigOpt: ensemble of Bayesian optimizers for picking hyperparmeters (more efficient than grid search). SigOptR is an R API for SigOpt. -Used unigrams, removed stopwords, and words that were seen in less than 5 docs\nOptimized for accuracy and coherence. LDA was way more accurate than LSA. Showed a accuracy vs topic coherence.\n\nTopic Modeling Consumer Complaints at the Federal Reserve, BJ Bloom, @bj_bloom\n\nGoal: Use consumer complaints to inform risk analysis for Federal Reserve Board\nUsed LDA topic modeling\nMain LDA packages in R: lda (old, fine), topicmodels (special snowflake, have to re-index to 0, tricky to use), stm (recommend, structural topic modeling, can use to incorporate metadata into topic model), textmineR (recommend, in his opinion includes diagnostic measures that make sense)\nIn his words, “Don’t use tm. It had its place back in the day.” Instead, he’s a fan of tidytext.\nOther R packages used: textrank (for pagerank)"
  },
  {
    "objectID": "posts/2019-11-12-dcr/index.html#spatial",
    "href": "posts/2019-11-12-dcr/index.html#spatial",
    "title": "DC R Conference Recap",
    "section": "Spatial Analysis",
    "text": "Spatial Analysis\nThe Care and Feeding of Spatial Data, Angela Li, @CivicAngela\n\nSlides available here\nShe described the steps before the fancy plots and regression, focusing on the data.\nTwo types of spatial data:\n\nVector data - points lines polygons (she focused more on this)\nRaster data - grids, pixels, cells\n\nGeo-coding: tmaptools::geocode_OSM(“Georgetown University”) gives coordinates (to translate human spatial data to computer understandable spatial data). Note that OSM = “open street map.” Talk to your GIS librarian (or if you have ARCGis) to get coords.\nIn her words: “Spatial file formats are confusing.”\nSpatial data formats: .shp (shapefile made up of 4 files), .geojson (lots of web-based mapping), .gpkg (new and good, but not common yet), .csv, .tiff (for raster data)\nLibrary she uses: sf package. st_read() or read_sf() to create a tidy sf data frame.\nHer tech stack: geoDa, QGIS, ArcGIS, PostGIS, CART, R\nR packages she uses the most (packages in parens are the “comparable” non-spatial versions):\n\nsf (readr, tidyr, dplyr)\ntmap (ggplot2)\nspdep (glmnet)\nsp (data.table)\n\nrayshader for 3D plotting.\n\nUse mapview for GIS-like experience\nThink beyond dots on a map. Are there clusters? Look at spatial auto-correlation and spatially constrained clustering.\nPre release of rgeoda on GitHub\nSee #rspatial on Twitter\nMost important slide she’s ever made in her life:\n\n\n\n\n\n\n\n\nR is Not Just Basic Stats: Think Spatially!, Tatyana Tsvetovat, @t_tsvet\n\nDivided household: She’s an R enthusiast and her husband is a Python fan 😱\nPackages she uses: rgdal, rgeos, sp, spatstat, kernlab, zoo"
  },
  {
    "objectID": "posts/2019-11-12-dcr/index.html#general",
    "href": "posts/2019-11-12-dcr/index.html#general",
    "title": "DC R Conference Recap",
    "section": "General R Programming",
    "text": "General R Programming\nTen Tremendous Tricks in the Tidyverse, David Robinson, @drob\nBe sue to catch his #tidytuesday screen casts on YouTube. Ten tricks that people have liked from his screen casts:\n\ncount(): One of the most used EDA functions\ncount(column, sort, wt, name): Create a new variable within count. For example: count(decade = 10 * (year %/% 10) to produce counts for each decade.\nadd_count() adds a count column (really useful to combine with filter to get x with a minimum count)\nUse summarize() to create a list column. Looks like data %&gt;% groupby() %&gt;% summarize. Example: summarize(test = list(t.test(avg_score))) Then use broom to visualize. See chapter 25 of R for Data Science.\nUse the combination of: count(), fct_reorder() (to turn column into ordered factor), geom_col (use instead of geom_bar(stat = ‘idendtity’)), coord_flip(),\n\nfct_lump() to combine least/most common levels together\nscale_x or y_log10() to use log scale. In his words: “I suspect much more data is log-normal than normal.”\ntidyr::crossing() to find all combinations of multiple vectors. Example: crossing(a = 1:3, b = A:Z, c = 10:20). Can use for simulations in place of for-loops.\nseparate() to split a column into multiple columns based on a regex\nextract() to extract information from a column based on a regex\n\n\n\n\nRaising baby with R, Jared Lander, @jaredlander\n\nJared and his wife recently had a baby. He described his use of R to choose the baby’s name and analyze its eating, sleeping, and poo patterns.\nSee the babynames package for data about popularity of baby names\nAnalyzing time series data: tsibble, fable, feast. Use model() function in this package to fit ts models. Use autoplot() to plot ts data. index_by() is the ts equiv of group_by().\nfable.prophet package to use prophet in a fable framework. Used this to perform change-point detection.\nSlides available here.\n\nR and Python coexisting in the same Development environment, Dan Chen, @chendaniely, (Author of “Pandas for Everyone”)\n\nWhat he believes Python does better: environments, web development, and hardware\nWhat he likes about R: communication (shiny, rmarkdown, ggplot2)\nPython environments in R:\n\n\nconda_envs &lt;- reticulate::cond_list()\nconda_envs$name[[1]] #have to restart R if you want to change env again)\n\n\nMost data scientists using Python use anaconda.\nDon’t mix anaconda R package installation with install.packages()\nNew model pipeline in R: rsample, recipes, parsnip, yardstick\n\nDescribed workflow: Python joblib for “pickling” a model in python. Read it into R. Then use R shiny to display\n[blogdown] supports jupyter notebooks\nknitpy: knitr implementation in python\nUse Apache arrow to pass datasets between R and python\nInformation on installing python here\nSlides available here\n\nManaging Your Cloud: Working with APIs, Marck Vaisman, @wahalulu\n\nDiscussed how to use R to interact with APIs (mostly for the consumption of data)\nplumber for building and serving up APIs\nthree package to be aware of: httr (Part of the tidyverse. More abstract/easier than curl package), curl (new package), jsonlite"
  },
  {
    "objectID": "posts/2019-11-12-dcr/index.html#random",
    "href": "posts/2019-11-12-dcr/index.html#random",
    "title": "DC R Conference Recap",
    "section": "A Few Remaining Random Bits",
    "text": "A Few Remaining Random Bits\n\nCheck out the memeR package by @sctyner\nR Markdown tips from @datadanya: Include session_info() at the end of all markdown docs. See https://bootswatch.com/3/ for a list of R Markdown themes\nUse gmailr to send email from R\nSee learnr for creating teaching materials. (Use in conjunction with gradethis\nFor conversations about R administration, see #radmins on Twitter, solutions.rstudio.com, or the radmins channel on community.rstudio.com. Management and IT folks should watch Kelly O’Briant’s talk, Reflections on a Year Spent Talking to Data Scientists About DevOps."
  },
  {
    "objectID": "posts/2020-02-07-shiny-r-ladies/index.html",
    "href": "posts/2020-02-07-shiny-r-ladies/index.html",
    "title": "Shiny R-Ladies",
    "section": "",
    "text": "Earlier this year I had the pleasure of talking about R shiny at the Baltimore R-Ladies Meetup. shiny is an R package for building interactive web applications.\nWe made an app showing the wildfires in south eastern Australia along with historic rainfall and temperature data. This dataset was provided as a Tidy Tuesday dataset by the R for Data Science Learning Community.\nHere is a peak at the app…\n\n\n\nAnd, a few more pictures from the event…\n\n\n\nSlides:"
  },
  {
    "objectID": "posts/2020-11-08-mirror-mirror-on-the-wall-does-my-test-data-reflect-my-training-data-at-all/index.html",
    "href": "posts/2020-11-08-mirror-mirror-on-the-wall-does-my-test-data-reflect-my-training-data-at-all/index.html",
    "title": "Mirror, mirror on the wall, does my test data reflect my training data at all?",
    "section": "",
    "text": "In the paper Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches, authors Dacrema, Cremonesi, and Jannach evaluate various neural networks which were designed for recommendation systems and proposed in prominent machine learning journals. A recommendation system is an information filtering system which uses user history (e.g. purchases, page views, clicks, thumbs up/down, etc) to provide personalized item recommendations to users.\nThe authors point out a few mistakes or “questionable techniques” used in the research they were attempting to reproduce and evaluate. The figure in the article snippet below points out that, for a particular model they were evaluating, the popularity distribution of items assigned to the test and train data splits did not seem to be the same. Interestingly, this affected the reported results and potentially the claim that the proposed model was a state-of-the-art! See the article for more details.\nSnippet of the 2020 paper, “Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches”, by Dacrema, Cremonesi, and Jannach\nI thought this was a nice example comparing the distribution of two samples (i.e. comparing the item popularity of the test set versus the training set). Why is this important? Well, because we would like to train our model using data similar to data that the model will score in the future. The test set is used to tell us how good (or bad) our model performs on data that it has not seen before.\nThe authors mentioned that they used the Gini index to evaluate the distribution of the test set. One might also wonder if a hypothesis test could be used to consider the evidence for or against the training and test sets having the same underlying distribution. Let’s look at both approaches."
  },
  {
    "objectID": "posts/2020-11-08-mirror-mirror-on-the-wall-does-my-test-data-reflect-my-training-data-at-all/index.html#the-data",
    "href": "posts/2020-11-08-mirror-mirror-on-the-wall-does-my-test-data-reflect-my-training-data-at-all/index.html#the-data",
    "title": "Mirror, mirror on the wall, does my test data reflect my training data at all?",
    "section": "The data",
    "text": "The data\nThe train and test data mentioned in the article above are available at github.com/lzheng21/SpectralCF. For convenience I also saved them as R dataframes in the data directory of the GitHub repo associated with this analysis. Using R, the data can be loaded as follows.\n\nload(\"data/train_test.RData\")\n\nLet’s take a peek at the training set. The data is composed of two columns containing user ids and item ids. It comes from MovieLens, which provides data about movie watching history. The existence of a user-item pair in the data means that the user interacted with the item (in this case the user watched the movie). A data scientist can use this information as “implicit feedback”, inferring that the user “liked” the item.\n\n\n\n\n\n\n  \n    \n      Sample of Training Data\n    \n    \n  \n  \n    \n      user\n      item\n    \n  \n  \n    1062\n538\n    589\n661\n    729\n127\n    2264\n78\n    1013\n2521\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n  \n    \n      Summary of Training Data\n    \n    \n  \n  \n    \n      Min User ID\n      Max User ID\n      User Count\n      Min Item ID\n      Max Item ID\n      Item Count\n    \n  \n  \n    0\n6013\n6014\n0\n3704\n3068\n  \n  \n  \n\n\n\n\nNext, count the number of user interactions per item in each of the train and test datasets. Only the first 6 rows of the result are shown in the table below.\n\nitem_frequency &lt;- \n  full_join(\n    train %&gt;% group_by(item) %&gt;% count() %&gt;% ungroup(),\n    test  %&gt;% group_by(item) %&gt;% count() %&gt;% ungroup(),\n    by = \"item\"\n  ) %&gt;%\n  rename(count_train = n.x, count_test = n.y) %&gt;%\n  replace_na(list(count_train = 0, count_test = 0))\n\n\n\n\n\n\n\n\n\n\nitem\nTrain Count\nTest Count\n\n\n\n\n0\n932\n5\n\n\n1\n77\n0\n\n\n2\n259\n1\n\n\n3\n308\n2\n\n\n4\n309\n85\n\n\n5\n1186\n0\n\n\n\n\n\n\n\nWe can use this information to re-create the plot from the paper:\n\n\nCode\nitem_frequency %&gt;%\n  mutate(train = count_train/max(count_train), test = count_test/max(count_test)) %&gt;%\n  arrange(desc(count_train)) %&gt;%\n  mutate(train_rank = row_number()) %&gt;%\n  select(-count_train, -count_test) %&gt;%\n  pivot_longer(-c(item, train_rank), names_to = \"data\", values_to = \"num_interactions\") %&gt;%\n  ggplot(aes(train_rank, num_interactions, group = data, color = data)) +\n  geom_line() +\n  geom_point() +\n  xlab(\"Item Index\") +\n  ylab(\"Normalized number of interactions\")\n\n\n\n\n\nFigure 1: Comparison of the provided test and train data\n\n\n\n\nI think that looks pretty close to the image in the article! As the authors point out, the items are sorted by their popularity in the training set (in descending order along the horizontal axis). The value of the (normalized) test popularity is also plotted for each item. We can see that the two distributions do not look similar. To be a bit more rigorous we can consider a few statistical procedures."
  },
  {
    "objectID": "posts/2020-11-08-mirror-mirror-on-the-wall-does-my-test-data-reflect-my-training-data-at-all/index.html#gini-index",
    "href": "posts/2020-11-08-mirror-mirror-on-the-wall-does-my-test-data-reflect-my-training-data-at-all/index.html#gini-index",
    "title": "Mirror, mirror on the wall, does my test data reflect my training data at all?",
    "section": "Gini Index",
    "text": "Gini Index\nThe article authors used the Gini index to evaluate the distribution of the provided test set. Let’s try it to see how it works. We’ll use the ineq R package.\n\n#install.packages(\"ineq\")\nlibrary(ineq)\n\nThe Gini index is a measure of inequality ranging from 0 (equality) to 1 (no equality). For example, if there were no popularity bias in the dataset (i.e. all items had the same number of interactions), then the Gini index would be 0. That’s not the case since some movies were watched more than others. Let’s look at the Gini index for the provided train and test sets.\n\n# Gini index for provided test set\ngini_provided_test &lt;- \n  item_frequency %&gt;% \n  select(-item, -count_train) %&gt;%\n  as.matrix() %&gt;%\n  ineq(type=\"Gini\")\n\n# Gini index for provided train set\ngini_provided_train &lt;- \n  item_frequency %&gt;% \n  select(-item, -count_test) %&gt;%\n  as.matrix() %&gt;%\n  ineq(type=\"Gini\")\n\ngini_provided_train\n\n[1] 0.7636983\n\ngini_provided_test\n\n[1] 0.9044503\n\n\nThe provided test set has a much higher Gini index than the train dataset (0.9 compared to 0.76). Is such a difference expected for a random train/test split? Keep reading."
  },
  {
    "objectID": "posts/2020-11-08-mirror-mirror-on-the-wall-does-my-test-data-reflect-my-training-data-at-all/index.html#resampling-the-test-data",
    "href": "posts/2020-11-08-mirror-mirror-on-the-wall-does-my-test-data-reflect-my-training-data-at-all/index.html#resampling-the-test-data",
    "title": "Mirror, mirror on the wall, does my test data reflect my training data at all?",
    "section": "Resampling the test data",
    "text": "Resampling the test data\nLet’s try sampling our own test set to see how that affects the Gini index. We’ll start with the full dataset, sampling user-item interactions at the same rate as the provided train/test split and stratifying by user.\n\n# Determine percentage of data that should be sampled for test\n(pct_test &lt;- nrow(test)/(nrow(test) + nrow(train)))\n\n[1] 0.2081349\n\n\n\n# Sample test set (stratify by user)\ndata_full &lt;- bind_rows(train,test)\ntest_new &lt;- \n  data_full %&gt;% \n  group_by(user) %&gt;%\n  slice_sample(prop = pct_test) %&gt;%\n  ungroup()\n\n# Put remaining data in train:\ntrain_new &lt;- anti_join(data_full, test_new)\n\n# Recreate our interaction counts for each item:\nitem_frequency_new &lt;- \n  full_join(\n    train_new %&gt;% group_by(item) %&gt;% count() %&gt;% ungroup(),\n    test_new  %&gt;% group_by(item) %&gt;% count() %&gt;% ungroup(),\n    by = \"item\"\n  ) %&gt;%\n  rename(count_train = n.x, count_test = n.y) %&gt;%\n  replace_na(list(count_train = 0, count_test = 0))\n\nLet’s visualize the newly sampled train and test data, which we can compare to Figure 1.\n\n\nCode\nitem_frequency_new %&gt;%\n  mutate(train = count_train/max(count_train), test = count_test/max(count_test)) %&gt;%\n  arrange(desc(count_train)) %&gt;%\n  mutate(train_rank = row_number()) %&gt;%\n  select(-count_train, -count_test) %&gt;%\n  pivot_longer(-c(item, train_rank), names_to = \"data\", values_to = \"num_interactions\") %&gt;%\n  ggplot(aes(train_rank, num_interactions, group = data, color = data)) +\n  geom_line() +\n  geom_point() +\n  xlab(\"Items\") +\n  ylab(\"Normalized number of interactions\")\n\n\n\n\n\nFigure 2: Comparison of our sampled test and train data\n\n\n\n\nThe test and train popularity distributions look much closer!"
  },
  {
    "objectID": "posts/2020-11-08-mirror-mirror-on-the-wall-does-my-test-data-reflect-my-training-data-at-all/index.html#gini-index-revisited",
    "href": "posts/2020-11-08-mirror-mirror-on-the-wall-does-my-test-data-reflect-my-training-data-at-all/index.html#gini-index-revisited",
    "title": "Mirror, mirror on the wall, does my test data reflect my training data at all?",
    "section": "Gini Index Revisited",
    "text": "Gini Index Revisited\nNow we can revisit the Gini index for the re-sampled train/test split.\n\n# Gini index for our sampled test set:\ngini_ours_test &lt;- \n  item_frequency_new %&gt;% \n  select(-item, -count_train) %&gt;%\n  as.matrix() %&gt;%\n  ineq(type=\"Gini\")\ngini_ours_test\n\n[1] 0.7549052\n\n# Gini index for our sampled train set:\ngini_ours_train &lt;- \n  item_frequency_new %&gt;% \n  select(-item, -count_test) %&gt;%\n  as.matrix() %&gt;%\n  ineq(type=\"Gini\")\n\ngini_ours_train\n\n[1] 0.7510046\n\n\nWe see that our sampled test set has a Gini index (0.75) that is much closer to that of our sampled training dataset (0.75).\nBut, how much difference can we expect between the two sampled datasets that are representative of the same distribution? Is the observed difference in the provided datasets likely due to random sampling or some other reason? Let’s take a look at a couple of hypothesis tests."
  },
  {
    "objectID": "posts/2020-11-08-mirror-mirror-on-the-wall-does-my-test-data-reflect-my-training-data-at-all/index.html#hypothesis-testing",
    "href": "posts/2020-11-08-mirror-mirror-on-the-wall-does-my-test-data-reflect-my-training-data-at-all/index.html#hypothesis-testing",
    "title": "Mirror, mirror on the wall, does my test data reflect my training data at all?",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\\(\\chi^2\\) Test of Homogeneity\n\nLet’s suppose that our null hypothesis is that the train and test sets have the same underlying distribution (vs the alternative hypothesis that they do not). We can consider the data as a 2 by 3232 contingency table (since there are 3232 items). One might naively choose to run a \\(\\chi^2\\) test to test this hypothesis.\n\n\n\nThe \\(\\chi^2\\) test is most appropriate when the data is not too sparse. Recall that one of the rule-of-thumb assumptions for the \\(\\chi^2\\) test is that no more than 20% of the expected cells counts are less than 5. \nLet’s check to see if this assumption is satisfied.\n\n# Check to see percentage of expected cell values are &lt; 5\n\n# Compute the expected values\nn_train &lt;- sum(item_frequency$count_train)\nn_test &lt;- sum(item_frequency$count_test) \nn &lt;- n_train + n_test\nitem_frequency %&lt;&gt;% \n  mutate(p = (count_train + count_test)/n,\n         e_test = p * n_test,\n         e_train = p * n_train)\n\n# Check what percentage of the expected values are less than 5\n(pct_expected_small &lt;- sum((item_frequency$e_test &lt; 5) + (item_frequency$e_test &lt; 5))/(2*nrow(item_frequency)))\n\n[1] 0.5792079\n\n\nWe see here that 57.9% of the expected values are very small (less than 5), meaning that the assumption in question is not satisfied.\n\nLet’s look at an alternative method.\n\n\nFisher’s Exact Test\nAgain consider a 2 by 3232 contingency table composed of the test and train vectors. We can utilize a test that is acceptable for small sample sizes: Fisher’s exact test can be used to test the null hypothesis that the two factors of a 2-dimensional contingency table are independent (no relationship) vs the alternative that they are not.\nConsider first Fisher’s exact test for the original train/test data:\n\nitem_frequency %&gt;% \n  select(count_train, count_test) %&gt;% \n  fisher.test(simulate.p.value = TRUE)\n\n\n    Fisher's Exact Test for Count Data with simulated p-value (based on\n    2000 replicates)\n\ndata:  .\np-value = 0.0004998\nalternative hypothesis: two.sided\n\n\nThe p-value is very small indicating that there is sufficient evidence that movie popularity is dependent on the dataset (test or train).\nNow consider Fisher’s exact test for our re-sampled train/test data:\n\nitem_frequency_new %&gt;% \n  select(count_train, count_test) %&gt;% \n  fisher.test(simulate.p.value = TRUE)\n\n\n    Fisher's Exact Test for Count Data with simulated p-value (based on\n    2000 replicates)\n\ndata:  .\np-value = 0.07596\nalternative hypothesis: two.sided\n\n\nIn this case the p-value is large meaning that we fail to reject the null hypothesis that movie popularity is independent of the dataset. In other words, we see a similar popularity pattern in both the test and train data sets that we sampled. These results align with our expectation."
  },
  {
    "objectID": "posts/2020-11-08-mirror-mirror-on-the-wall-does-my-test-data-reflect-my-training-data-at-all/index.html#closing",
    "href": "posts/2020-11-08-mirror-mirror-on-the-wall-does-my-test-data-reflect-my-training-data-at-all/index.html#closing",
    "title": "Mirror, mirror on the wall, does my test data reflect my training data at all?",
    "section": "Closing Thoughts",
    "text": "Closing Thoughts\nUsing a few methods (vizualization, Gini index, and hypothesis testing) we observed that the provided test data set did not follow the same popularity distribution as the training set.\nAnother scenario to consider: Should we always expect our train and test data to have the same distribution? What if the test/train split were based on time (i.e. we use earlier data to train the model and use the most recent data to test). How much difference is acceptable? After all, movie popularity changes over time (thinking about our particular application). This might be an interesting topic for a future blog post.\n\nIf you would like to provide feedback on this blog post, you can contact me via Twitter (@DrAmandaRP). Thanks for reading!"
  },
  {
    "objectID": "posts/2021-01-21-rstudio-global/index.html",
    "href": "posts/2021-01-21-rstudio-global/index.html",
    "title": "rstudio::global(2021) |> summarize()",
    "section": "",
    "text": "Due to the COVID-19 pandemic, this year’s rstudio::conf was virtual (and free!). The conference is about all things R and RStudio. There were 15K+ registrants. Talks ran concurrently over a 12 hour period (and repeated again for viewers in other time zones). My notes below are only for a handful of selected talks– there are many others that I was not able to watch (yet). Videos are available on the RStuduio website."
  },
  {
    "objectID": "posts/2021-01-21-rstudio-global/index.html#keynote-maintaining-the-house-that-tidyverse-built",
    "href": "posts/2021-01-21-rstudio-global/index.html#keynote-maintaining-the-house-that-tidyverse-built",
    "title": "rstudio::global(2021) |> summarize()",
    "section": "Keynote: Maintaining the house that tidyverse built",
    "text": "Keynote: Maintaining the house that tidyverse built\nSpeaker: Hadley Wickham (Chief Scientist at RStudio)\n\n\n\n\n\n\nDiscussed the life cycle of functions and packages, which you will see tagged on many of RStudio’s packages. See this page and the lifecycle package for more details.\nTo maintain a static package installation for a project: Use renv or RStudio’s public package manager. On the latter, just pick a day in the past to maintain package state from that date (only for packages on CRAN).\nIf you haven’t already heard, dplyr’s spread & gather functions are not going away. However, the recommended replacement functions are pivot_longer and wider_wider.\n\n\n\n\n\n\n\nmagrittr will be around for at least 4 or 5 years, but people will be encouraged to use the new pipe in base R version 4.1.\nmodelr is superseded by tidymodels."
  },
  {
    "objectID": "posts/2021-01-21-rstudio-global/index.html#r-python-going-steady",
    "href": "posts/2021-01-21-rstudio-global/index.html#r-python-going-steady",
    "title": "rstudio::global(2021) |> summarize()",
    "section": "R & Python: Going Steady",
    "text": "R & Python: Going Steady\nSpeaker: Sean Lopp (Project Manager at RStudio)\n\nMyth: Data science teams need to choose between R and Python (or some other language).\nTruth: Using the right tool for the right job can make data science teams more effective.\nRStudio Server Pro: Provides a common architecture for many different types of data science tools (e.g. Jupyter notebooks, JupyterLab, RStudio). IT teams only need to set up one infrastructure instead of supporting many tools.\nRStudio Connect: Provides a common hosting architecture for many different types of data science products (e.g. reports, notebooks, APIs, shiny applications). Additionally, RStudio Connect increases collaboration between team members using different programming languages and tools.\nMy thoughts: I would ❤️ to have access to RStudio Connect."
  },
  {
    "objectID": "posts/2021-01-21-rstudio-global/index.html#art-lessons-one-year-as-rstudios-artist-in-residence",
    "href": "posts/2021-01-21-rstudio-global/index.html#art-lessons-one-year-as-rstudios-artist-in-residence",
    "title": "rstudio::global(2021) |> summarize()",
    "section": "Art Lessons: One Year as RStudio’s Artist-in-Residence",
    "text": "Art Lessons: One Year as RStudio’s Artist-in-Residence\nSpeaker: Allison Horst (Professor at the Bren School of Environmental Science and Management at UC Santa Barbara)\nAllison talked about the importance of art for making R instruction material more approachable for learners. Some of her new artwork was featured in this talk– I’m feeling motivated to add some of new artwork to some of my training materials. Allison’s artwork can be found in her GitHub repo."
  },
  {
    "objectID": "posts/2021-01-21-rstudio-global/index.html#keynote-reporting-on-and-visualising-the-pandemic",
    "href": "posts/2021-01-21-rstudio-global/index.html#keynote-reporting-on-and-visualising-the-pandemic",
    "title": "rstudio::global(2021) |> summarize()",
    "section": "Keynote: Reporting on and visualising the pandemic",
    "text": "Keynote: Reporting on and visualising the pandemic\nSpeaker: John Burn-Murdoch (Financial Timesâ senior data visualization journalist, and creator of the FTâ’s coronavirus trajectory tracker charts)\nJohn’s talk focused on data visualization for a mass audience. The data viz research paper that has been the most influential on his work: Beyond Memorability: Visualization Recognition and Recall.\n\n\n\n\n\nLessons:\n\nText and other annotations are critical when sharing charts to the masses. The annotations make the chart accessible.\nIt’s the responsibility of the chart designer to make sure the reader is not confused when looking at the chart.\nInfo viz is personal and often political. People will bring their prior beliefs to the plot and make judgments about the utility of the chart (for example many strong opinions about the use of log scale on the vertical axis).\nDon’t publish and vanish. Incorporate readers’ feedback.\nEase of understanding is top priority (more than accuracy). For example, using a 7 day rolling average may be easier for people to understand than splines (even though the latter shows more accurate numbers).\nAnimation can be incredibly effective (but use it sparingly)."
  },
  {
    "objectID": "posts/2021-01-21-rstudio-global/index.html#whats-new-in-tidymodels",
    "href": "posts/2021-01-21-rstudio-global/index.html#whats-new-in-tidymodels",
    "title": "rstudio::global(2021) |> summarize()",
    "section": "What’s new in tidymodels?",
    "text": "What’s new in tidymodels?\nSpeaker: Max Kuhn (Software engineer at RStudio)\nRecent updates in tidymodels:\n\nAdded limited support for sparse matrices.\nfinetune package (extension of tune package) for finding optimal tuning hyper-parameters. Nice methods available in finetune finding optimal hyper-parameters (methods are more efficient that full grid search):\n\nRacing methods\nSimulated Annealing search\n\n\nHow do you pick a modeling framework (e.g. mlr3, h2o, tidymodels)?\n\nh2o for speed (sub-second latency).\nIf your framework is tidyverse, you might go with tidymodels.\nmlr3 is great, too 😃\n\nSee the book: Tidy modeling with R. Slides for talk available here."
  },
  {
    "objectID": "posts/2021-01-21-rstudio-global/index.html#feedback-at-scale",
    "href": "posts/2021-01-21-rstudio-global/index.html#feedback-at-scale",
    "title": "rstudio::global(2021) |> summarize()",
    "section": "Feedback at scale",
    "text": "Feedback at scale\nSpeaker: Mine Çetinkaya-Rundel (Educator and Data Scientist at RStudio, Senior Lecturer in the School of Mathematics at University of Edinburgh)\nBuilding tutorials using learnr:\n\nReally shines with coding exercises. Provides instant feedback to students.\nRecommendations: Include starter code for students to complete. Give “human friendly” feedback in response to incorrect responses. Check the result instead of checking the code (since students can order operations in multiple ways).\n\nlearnrhash: Use with learnr to collect (hashed) answer submissions from student learnr sessions.\nOptions for distributing at scale:\n\nshinyapps.io or RStudio connect, or\n\ndistribute within a package"
  },
  {
    "objectID": "posts/2021-01-21-rstudio-global/index.html#always-look-on-the-bright-side-of-plots",
    "href": "posts/2021-01-21-rstudio-global/index.html#always-look-on-the-bright-side-of-plots",
    "title": "rstudio::global(2021) |> summarize()",
    "section": "Always look on the bright side of plots",
    "text": "Always look on the bright side of plots\nSpeaker: Kara Woo (Research scientist in data curation at Sage Bionetworks)\nDiscussion points: Mapping mishaps, scale snafus (setting scale limits removes data before statistical summaries. To zoom in on a plot, better to set coordinate limits), and theme threats. Kara talked about common mistakes that people make when using ggplot2.\nFollow accidental aRt on Twitter."
  },
  {
    "objectID": "posts/2021-04-03-book-recommendations-using-neural-collaborative-filtering/index.html",
    "href": "posts/2021-04-03-book-recommendations-using-neural-collaborative-filtering/index.html",
    "title": "Book Recommendations via Neural Collaborative Filtering",
    "section": "",
    "text": "Over the past 1.5 years I have enjoyed being part of a book club with ladies from my church. Sadly our book club ended as life has pulled us in separate directions. I plan to continue reading books, but what should I read? I decided to build a recommender system."
  },
  {
    "objectID": "posts/2021-04-03-book-recommendations-using-neural-collaborative-filtering/index.html#filtering-by-genre",
    "href": "posts/2021-04-03-book-recommendations-using-neural-collaborative-filtering/index.html#filtering-by-genre",
    "title": "Book Recommendations via Neural Collaborative Filtering",
    "section": "Filtering by genre",
    "text": "Filtering by genre\nThe shelves field in the book_info data set is a list of virtual shelves that users have added books to. The shelf name is defined by the user and can be anything such as “to read”, “book club”, or the name of the genre. By looking at the list of shelves associated with a particular book, we can make some inferences about its genre. To limit books for my book club to Christian fiction (the focus of our group), I included books that were on at least one shelf with “christian” in the name and at least one shelf with “fiction” in the name (being careful to ignore “nonfiction” and “non-fiction”). This isn’t a perfect method for determining genre, but it significantly reduced the data size: 228,648,342 user-book interactions reduced to 5,628,062, a 98% reduction. If you’d like to try filtering the data by a different genre, use the genre_subset.R script.\n\n    \nThe top five books appearing on the most shelves\n\nThe top book, The Lion, the Witch and the Wardrobe, is a well known classic fiction book and part of the series of books at #3, The Chronicles of Narnia. It is one that I read as a young girl. I was not familiar with the books at #2 and #4: The Poisonwood Bible and The Five People You Meet in Heaven. Finally, the last on the list at #5 is Unbroken: A World War II Story of Survival, Resilience, and Redemption. This book is a true story and actually shouldn’t be included in the fiction genre. So, we see that this filtering method isn’t perfect. I’m OK with that."
  },
  {
    "objectID": "posts/2021-04-03-book-recommendations-using-neural-collaborative-filtering/index.html#bookmark",
    "href": "posts/2021-04-03-book-recommendations-using-neural-collaborative-filtering/index.html#bookmark",
    "title": "Book Recommendations via Neural Collaborative Filtering",
    "section": "Inferring Likes and Dislikes",
    "text": "Inferring Likes and Dislikes\nFor the NCF model, we need a list of books that each user liked and a list that they did not like. Some users provided explicit ratings (1 to 5 stars), however, not all users rate the books that they read. Additionally, it may be the case that some users just rate books that they like (or didn’t like). To obtain a sufficient amount of training data, I included both explicit and implicit feedback.\nWe’ll assume that a reader likes the book if:\n\nThey read the book AND (either gave it 4 or 5 stars OR didn’t provide rating). That is: (is_read == 1 & (0 == rating | rating &gt;= 4)), OR\nThe book is on their shelf and not read yet. That is, (is_read == 0).\n\nThis is an optimistic viewpoint because we may include books on the reader’s shelf that they later read and decide that they do not like.\nAlternatively, we’ll assume that a reader did not like a book if they read it and gave it a rating of 1 or 2. Books with a score of 3 were not used for model training.\nNote that there exist cases where no rating was provided (value of rating is 0), but a text review exists. An enhancement for obtaining more positive and negative examples might be to mine the sentiment of reviews for these instances.\nAfter performing these steps, I filtered out users who did not have at least 3 “positive” book examples (a threshold that could be tweaked). As a result we have a total of 264898 users and 25450 books."
  },
  {
    "objectID": "posts/2021-04-03-book-recommendations-using-neural-collaborative-filtering/index.html#data-for-my-book-club",
    "href": "posts/2021-04-03-book-recommendations-using-neural-collaborative-filtering/index.html#data-for-my-book-club",
    "title": "Book Recommendations via Neural Collaborative Filtering",
    "section": "Data for my book club",
    "text": "Data for my book club\nI added our club’s books to the data set. We have read dozens of books, but the following were those that we had a strong opinion about (either “good” and “bad”).\n\n         Books that we liked\n\n\n   \nBooks that we did not like\n\nAs you can see, the number of “duds” was much smaller than the number of books that we liked. Francine Rivers was a popular author (I’ve only read one of her books, but hope to catch up soon). I would have also used the following books as positive training examples, but they were not in the data set: The Kremlin Conspiracy by Joel Rosenberg  and My Heart Belongs in Blue Ridge by Pepper Basham."
  },
  {
    "objectID": "posts/2021-04-03-book-recommendations-using-neural-collaborative-filtering/index.html#trainvalidationtest-split",
    "href": "posts/2021-04-03-book-recommendations-using-neural-collaborative-filtering/index.html#trainvalidationtest-split",
    "title": "Book Recommendations via Neural Collaborative Filtering",
    "section": "Train/Validation/Test Split",
    "text": "Train/Validation/Test Split\nI split the data into train, validation, and test sets following the method used in the NCF paper:\n\nTraining: 4 negatives for every positive (for each user)\nValidation: 1 positive (for each user)\nTest: 1 positive, 100 negative (for each user)\n\nIf a user did not have enough negatives for the 4-to-1 ratio of negatives to positives in the training set, then I sampled “implicit” negatives (i.e. those books that are not on the user’s shelf). The empirical popularity distribution of the books provided sampling probabilities, meaning that more popular books were more likely to be sampled. This is often a good idea for training recommender systems since users are likely already aware of the popular items. I sampled these training data points for each epoch of model training.\nIf the user had more negatives than the 4-to-1 ratio, then the extras were put in the test set. Subsequently, if the test set lacked the 100 negatives, then I sampled “implicit” negatives to fill out the test set (using the same strategy described for the training set). Sampling of the test set was done once prior to model training."
  },
  {
    "objectID": "posts/2021-04-03-book-recommendations-using-neural-collaborative-filtering/index.html#recommendations-for-my-book-club",
    "href": "posts/2021-04-03-book-recommendations-using-neural-collaborative-filtering/index.html#recommendations-for-my-book-club",
    "title": "Book Recommendations via Neural Collaborative Filtering",
    "section": "Recommendations for my book club",
    "text": "Recommendations for my book club\nTo get a better sense of how this recommender did, let’s inspect the recommendations for my book club. The top 3 recommendations were all books by Francine Rivers (which makes sense since four of her books were on our short list of books that we liked). In fact, looking at the top 25 books on the recommended list, 76 percent were written by this author. It seems that having four of nine books by Francine as part of our “good” training set may be skewing the recommendations toward this author.\nComing in at #4, we find the first book by a different author: Gods and Kings by Lynn Austin. This is an author that we have not read before. Interestingly, the book appears as a “Readers also enjoyed” suggestion on the GoodReads page of one of our “liked” books:\n\n\n\n\n\n\n\n\n\nLooking at the worst scoring books for our group, we find books that we would likely not enjoy: Come On Over (CO2) and Marry Me Now, Sorry Later (rank 25,437 and 25,436 respectively). In fact, these books are not even in our preferred genre of Christian fiction. They were mistakenly included due to the author’s name: Christian Simamora.\nNext, I decided to look at the rank of the top recommended book for each of the authors who wrote books that my group enjoyed. The result was (out of 25,437 total books):\n\nFrancine Rivers (#1, Mark of the Lion Trilogy)\nLori Wick (#30, Pretense)\nCharles Martin (#32, Wrapped in Rain)\nTessa Afshar (#38, In the Field of Grace)\nRandy Alcorn (#425, Deadline)\n\nInterestingly, I had already ordered Tessa Afshar’s In the Field of Grace before looking at these recommendations. So, that is right on point. Sadly, Randy Alcorn’s highest ranking book is at number 425 (his book, Safely Home, was one of my favorites).\nOverall, I’m fairly happy with these recommendations, even though the top of the list doesn’t demonstrate as much author variety as I’d like to see. The recommendations do feel tailored to my group’s reading interests."
  }
]